<h1>NAME</h1>
<p>nvidia-smi - NVIDIA System Management Interface program</p>
<h1>SYNOPSIS</h1>
<p>nvidia-smi [OPTION1 [ARG1]] [OPTION2 [ARG2]] ...</p>
<h1>DESCRIPTION</h1>
<p>nvidia-smi (also NVSMI) provides monitoring and management
capabilities for each of NVIDIA's Tesla, Quadro, GRID and GeForce
devices from Fermi and higher architecture families. GeForce Titan
series devices are supported for most functions with very limited
information provided for the remainder of the Geforce brand. NVSMI is a
cross platform tool that supports all standard NVIDIA driver-supported
Linux distros, as well as 64bit versions of Windows starting with
Windows Server 2008 R2. Metrics can be consumed directly by users via
stdout, or provided by file via CSV and XML formats for scripting
purposes.</p>
<p>Note that much of the functionality of NVSMI is provided by the
underlying NVML C-based library. See the NVIDIA developer website link
below for more information about NVML. NVML-based python bindings are
also available.</p>
<p>The output of NVSMI is not guaranteed to be backwards compatible.
However, both NVML and the Python bindings are backwards compatible, and
should be the first choice when writing any tools that must be
maintained across NVIDIA driver releases.</p>
<p><strong>NVML SDK:</strong>
<em>https://docs.nvidia.com/deploy/nvml-api/index.html</em></p>
<p><strong>Python bindings:</strong>
<em>http://pypi.python.org/pypi/nvidia-ml-py/</em></p>
<h1>OPTIONS</h1>
<h2>GENERAL OPTIONS</h2>
<h2>-h, --help</h2>
<p>Print usage information and exit.</p>
<h2>--version</h2>
<p>Print version information and exit.</p>
<h2>LIST OPTIONS</h2>
<h2>-L, --list-gpus</h2>
<p>List each of the NVIDIA GPUs in the system, along with their
UUIDs.</p>
<h2>-B, --list-excluded-gpus</h2>
<p>List each of the excluded NVIDIA GPUs in the system, along with their
UUIDs.</p>
<h2>SUMMARY OPTIONS</h2>
<h2>Show a summary of GPUs connected to the system.</h2>
<h2>-col, --columns</h2>
<p>Show a summary of GPUs connected to the system in a multi-column
format.</p>
<h2>[any one of]</h2>
<h2>-i, --id=ID</h2>
<p>Target a specific GPU.</p>
<h2>-f FILE, --filename=FILE</h2>
<p>Log to the specified file, rather than to stdout.</p>
<h2>-l SEC, --loop=SEC</h2>
<p>Probe until Ctrl+C at specified second interval.</p>
<h2>QUERY OPTIONS</h2>
<h2>-q, --query</h2>
<p>Display GPU or Unit info. Displayed info includes all data listed in
the (<em>GPU ATTRIBUTES</em>) or (<em>UNIT ATTRIBUTES</em>) sections of
this document. Some devices and/or environments don't support all
possible information. Any unsupported data is indicated by a "N/A" in
the output. By default information for all available GPUs or Units is
displayed. Use the <strong>-i</strong> option to restrict the output to
a single GPU or Unit.</p>
<h2>[plus optionally]</h2>
<h2>-u, --unit</h2>
<p>Display Unit data instead of GPU data. Unit data is only available
for NVIDIA S-class Tesla enclosures.</p>
<h2>-i, --id=ID</h2>
<p>Display data for a single specified GPU or Unit. The specified id may
be the GPU/Unit's 0-based index in the natural enumeration returned by
the driver, the GPU's board serial number, the GPU's UUID, or the GPU's
PCI bus ID (as domain:bus:device.function in hex). It is recommended
that users desiring consistency use either UUID or PCI bus ID, since
device enumeration ordering is not guaranteed to be consistent between
reboots and board serial number might be shared between multiple GPUs on
the same board.</p>
<h2>-f FILE, --filename=FILE</h2>
<p>Redirect query output to the specified file in place of the default
stdout. The specified file will be overwritten.</p>
<h2>-x, --xml-format</h2>
<p>Produce XML output in place of the default human-readable format.
Both GPU and Unit query outputs conform to corresponding DTDs. These are
available via the <strong>--dtd</strong> flag.</p>
<h2>--dtd</h2>
<p>Use with <strong>-x</strong>. Embed the DTD in the XML output.</p>
<h2>--debug=FILE</h2>
<p>Produces an encrypted debug log for use in submission of bugs back to
NVIDIA.</p>
<h2>-d TYPE, --display=TYPE</h2>
<p>Display only selected information: MEMORY, UTILIZATION, ECC,
TEMPERATURE, POWER, CLOCK, COMPUTE, PIDS, PERFORMANCE, SUPPORTED_CLOCKS,
PAGE_RETIREMENT, ACCOUNTING, ENCODER_STATS, SUPPORTED_GPU_TARGET_TEMP,
VOLTAGE, FBC_STATS, ROW_REMAPPER, GSP_FIRMWARE_VERSION, POWER_SMOOTHING,
POWER_PROFILES. Flags can be combined with comma e.g. "MEMORY,ECC".
Sampling data with max, min and avg is also returned for POWER,
UTILIZATION and CLOCK display types. Doesn't work with -u/--unit or
-x/--xml-format flags.</p>
<h2>-l SEC, --loop=SEC</h2>
<p>Continuously report query data at the specified interval, rather than
the default of just once. The application will sleep in-between queries.
Note that on Linux ECC error or Xid error events will print out during
the sleep period if the -x flag was not specified. Pressing Ctrl+C at
any time will abort the loop, which will otherwise run indefinitely. If
no argument is specified for the <strong>-l</strong> form a default
interval of 5 seconds is used.</p>
<h2>-lms ms, --loop-ms=ms</h2>
<p>Same as -l,--loop but in milliseconds.</p>
<h2>SELECTIVE QUERY OPTIONS</h2>
<p>Allows the caller to pass an explicit list of properties to
query.</p>
<h2>[one of]</h2>
<h2>--query-gpu=</h2>
<p>Information about GPU. Pass comma separated list of properties you
want to query. e.g. --query-gpu=pci.bus_id,persistence_mode. Call
--help-query-gpu for more info.</p>
<h2>--query-supported-clocks=</h2>
<p>List of supported clocks. Call --help-query-supported-clocks for more
info.</p>
<h2>--query-compute-apps=</h2>
<p>List of currently active compute processes. Call
--help-query-compute-apps for more info.</p>
<h2>--query-accounted-apps=</h2>
<p>List of accounted compute processes. Call --help-query-accounted-apps
for more info. This query is not supported on vGPU host.</p>
<h2>--query-retired-pages=</h2>
<p>List of GPU device memory pages that have been retired. Call
--help-query-retired-pages for more info.</p>
<h2>--query-remapped-rows=</h2>
<p>Information about remapped rows. Call --help-query-remapped-rows for
more info.</p>
<h2>[mandatory]</h2>
<h2>--format=</h2>
<p>Comma separated list of format options:</p>
<ul>
<li><p>csv - comma separated values (MANDATORY)</p></li>
</ul>
<ul>
<li><p>noheader - skip first line with column headers</p></li>
</ul>
<ul>
<li><p>nounits - don't print units for numerical values</p></li>
</ul>
<h2>[plus any of]</h2>
<h2>-i, --id=ID</h2>
<p>Display data for a single specified GPU. The specified id may be the
GPU's 0-based index in the natural enumeration returned by the driver,
the GPU's board serial number, the GPU's UUID, or the GPU's PCI bus ID
(as domain:bus:device.function in hex). It is recommended that users
desiring consistency use either UUID or PCI bus ID, since device
enumeration ordering is not guaranteed to be consistent between reboots
and board serial number might be shared between multiple GPUs on the
same board.</p>
<h2>-f FILE, --filename=FILE</h2>
<p>Redirect query output to the specified file in place of the default
stdout. The specified file will be overwritten.</p>
<h2>-l SEC, --loop=SEC</h2>
<p>Continuously report query data at the specified interval, rather than
the default of just once. The application will sleep in-between queries.
Note that on Linux ECC error or Xid error events will print out during
the sleep period if the -x flag was not specified. Pressing Ctrl+C at
any time will abort the loop, which will otherwise run indefinitely. If
no argument is specified for the <strong>-l</strong> form a default
interval of 5 seconds is used.</p>
<h2>-lms ms, --loop-ms=ms</h2>
<p>Same as -l,--loop but in milliseconds.</p>
<h2>DEVICE MODIFICATION OPTIONS</h2>
<h2>[any one of]</h2>
<h2>-pm, --persistence-mode=MODE</h2>
<p>Set the persistence mode for the target GPUs. See the (<em>GPU
ATTRIBUTES</em>) section for a description of persistence mode. Requires
root. Will impact all GPUs unless a single GPU is specified using the -i
argument. The effect of this operation is immediate. However, it does
not persist across reboots. After each reboot persistence mode will
default to "Disabled". Available on Linux only.</p>
<h2>-e, --ecc-config=CONFIG</h2>
<p>Set the ECC mode for the target GPUs. See the (<em>GPU
ATTRIBUTES</em>) section for a description of ECC mode. Requires root.
Will impact all GPUs unless a single GPU is specified using the -i
argument. This setting takes effect after the next reboot and is
persistent.</p>
<h2>-p, --reset-ecc-errors=TYPE</h2>
<p>Reset the ECC error counters for the target GPUs. See the (<em>GPU
ATTRIBUTES</em>) section for a description of ECC error counter types.
Available arguments are 0\|VOLATILE or 1\|AGGREGATE. Requires root. Will
impact all GPUs unless a single GPU is specified using the -i argument.
The effect of this operation is immediate. Clearing aggregate counts is
not supported on Ampere+</p>
<h2>-c, --compute-mode=MODE</h2>
<p>Set the compute mode for the target GPUs. See the (<em>GPU
ATTRIBUTES</em>) section for a description of compute mode. Requires
root. Will impact all GPUs unless a single GPU is specified using the -i
argument. The effect of this operation is immediate. However, it does
not persist across reboots. After each reboot compute mode will reset to
"DEFAULT".</p>
<h2>-dm TYPE, --driver-model=TYPE</h2>
<h2>-fdm TYPE, --force-driver-model=TYPE</h2>
<p>Enable or disable TCC driver model. For Windows only. Requires
administrator privileges. -dm will fail if a display is attached, but
-fdm will force the driver model to change. Will impact all GPUs unless
a single GPU is specified using the -i argument. A reboot is required
for the change to take place. See <strong>Driver Model</strong> for more
information on Windows driver models. An error message indicates that
retrieving the field failed.</p>
<h2>--gom=MODE</h2>
<p>Set GPU Operation Mode: 0/ALL_ON, 1/COMPUTE, 2/LOW_DP Supported on
GK110 M-class and X-class Tesla products from the Kepler family. Not
supported on Quadro and Tesla C-class products. LOW_DP and ALL_ON are
the only modes supported on GeForce Titan devices. Requires
administrator privileges. See <em>GPU Operation Mode</em> for more
information about GOM. GOM changes take effect after reboot. The reboot
requirement might be removed in the future. Compute only GOMs don't
support WDDM (Windows Display Driver Model)</p>
<h2>-r, --gpu-reset</h2>
<p>Trigger a reset of one or more GPUs. Can be used to clear GPU HW and
SW state in situations that would otherwise require a machine reboot.
Typically useful if a double bit ECC error has occurred. Optional -i
switch can be used to target one or more specific devices. Without this
option, all GPUs are reset. Requires root. There can't be any
applications using these devices (e.g. CUDA application, graphics
application like X server, monitoring application like other instance of
nvidia-smi). There also can't be any compute applications running on any
other GPU in the system if individual GPU reset is not feasible.</p>
<p>Starting with the NVIDIA Ampere architecture, GPUs with NVLink
connections can be individually reset. On Ampere NVSwitch systems,
Fabric Manager is required to facilitate reset. On Hopper and later
NVSwitch systems, the dependency on Fabric Manager to facilitate reset
is removed.</p>
<p>If Fabric Manager is not running, or if any of the GPUs being reset
are based on an architecture preceding the NVIDIA Ampere architecture,
any GPUs with NVLink connections to a GPU being reset must also be reset
in the same command. This can be done either by omitting the -i switch,
or using the -i switch to specify the GPUs to be reset. If the -i option
does not specify a complete set of NVLink GPUs to reset, this command
will issue an error identifying the additional GPUs that must be
included in the reset command.</p>
<p>Specific details are outlined in the tables below:</p>
<p>NVSwitch systems:</p>
<pre><code> GPU Family | Fabric Manager running       | Fabric Manager not running   
------------|------------------------------|------------------------------
 Pre-Ampere | All PEER connected GPUs must | All PEER connected GPUs must 
            | be reset in same command.    | be reset in same command     
 Ampere+    | Each GPU can be reset        | All PEER connected GPUs must 
            | individually                 | be reset in same command     
</code></pre>
<p>Direct connected NVLink systems: (FM is not supported, as no NVSwitch
HW is present)</p>
<pre><code> GPU Family | Capabilities                                          
------------|-------------------------------------------------------
 Pre-Ampere | All PEER connected GPUs must be reset in same command 
 Ampere+    | Each GPU can be reset individually                    
</code></pre>
<p>GPU reset is not guaranteed to work in all cases. It is not
recommended for production environments at this time. In some situations
there may be HW components on the board that fail to revert back to an
initial state following the reset request. This is more likely to be
seen on Fermi-generation products vs. Kepler, and more likely to be seen
if the reset is being performed on a hung GPU.</p>
<p>Following a reset, it is recommended that the health of each reset
GPU be verified before further use. If any GPU is not healthy a complete
reset should be instigated by power cycling the node.</p>
<p>Reset triggered without extra arguments, will be a default Function
Level Reset (FLR). To issue a Bus Reset, use -r bus. For certain
platforms only Function Level Reset is possible.</p>
<p>GPU reset operation will not be supported on MIG enabled vGPU
guests.</p>
<p>Visit <em>http://developer.nvidia.com/gpu-deployment-kit</em> to
download the GDK.</p>
<h2>-vm, --virt-mode=MODE</h2>
<p>Switch GPU Virtualization Mode. Sets GPU virtualization mode to
3/VGPU or 4/VSGA. Virtualization mode of a GPU can only be set when it
is running on a hypervisor.</p>
<h2>-lgc, --lock-gpu-clocks=MIN_GPU_CLOCK,MAX_GPU_CLOCK</h2>
<p>Specifies &lt;minGpuClock,maxGpuClock&gt; clocks as a pair (e.g.
1500,1500) that defines closest desired locked GPU clock speed in MHz.
Input can also use be a singular desired clock value (e.g.
&lt;GpuClockValue&gt;). Optionally, --mode can be supplied to specify
the clock locking modes. Supported on Volta+. Requires root.</p>
<dl>
<dt><strong>--mode=0 (Default)</strong></dt>
<dd>
<p>This mode is the default clock locking mode and provides the highest
possible frequency accuracies supported by the hardware.</p>
</dd>
</dl>
<dl>
<dt><strong>--mode=1</strong></dt>
<dd>
<p>The clock locking algorithm leverages close loop controllers to
achieve frequency accuracies with improved perf per watt for certain
class of applications. Due to convergence latency of close loop
controllers, the frequency accuracies may be slightly lower than default
mode 0.</p>
</dd>
</dl>
<h2>-lmc, --lock-memory-clocks=MIN_MEMORY_CLOCK,MAX_MEMORY_CLOCK</h2>
<p>Specifies &lt;minMemClock,maxMemClock&gt; clocks as a pair (e.g.
5100,5100) that defines the range of desired locked Memory clock speed
in MHz. Input can also be a singular desired clock value (e.g.
&lt;MemClockValue&gt;). Requires root. <strong>Note: this option does
not work on GPUs based on NVIDIA Hopper architectures; to lock memory
clocks on those systems use --lock-memory-clocks-deferred
instead.</strong></p>
<h2>-rgc, --reset-gpu-clocks</h2>
<p>Resets the GPU clocks to the default value. Supported on Volta+.
Requires root.</p>
<h2>-rmc, --reset-memory-clocks</h2>
<p>Resets the memory clocks to the default value. Supported on Volta+.
Requires root.</p>
<h2>-ac, --applications-clocks=MEM_CLOCK,GRAPHICS_CLOCK</h2>
<p>This option is deprecated and will be removed in in a future CUDA
release. Please use -lmc for locking memory clocks and -lgc for locking
graphics clocks. Specifies maximum &lt;memory,graphics&gt; clocks as a
pair (e.g. 2000,800) that defines GPU's speed while running applications
on a GPU. Supported on Maxwell-based GeForce and from the Kepler+ family
in Tesla/Quadro/Titan devices. Requires root.</p>
<h2>-rac, --reset-applications-clocks</h2>
<p>This option is deprecated and will be removed in in a CUDA future
release. Resets the applications clocks to the default value. Supported
on Maxwell-based GeForce and from the Kepler+ family in
Tesla/Quadro/Titan devices. Requires root.</p>
<h2>-lmcd, --lock-memory-clocks-deferred</h2>
<p>Specifies the memory clock that defines the closest desired Memory
Clock in MHz. The memory clock takes effect the next time the GPU is
initialized. This can be guaranteed by unloading and reloading the
kernel module. Requires root.</p>
<h2>-rmcd, --reset-memory-clocks-deferred</h2>
<p>Resets the memory clock to default value. Driver unload and reload is
required for this to take effect. This can be done by unloading and
reloading the kernel module. Requires root.</p>
<h2>-pl, --power-limit=POWER_LIMIT</h2>
<p>Specifies maximum power limit in watts. Accepts integer and floating
point numbers. it takes an optional argument --scope. Only on supported
devices from Kepler family. Value needs to be between Min and Max Power
Limit as reported by nvidia-smi. Requires root.</p>
<h2>-sc, --scope=0/GPU, 1/TOTAL_MODULE</h2>
<p>Specifies the scope of the power limit. Following are the options:
0/GPU: This only changes power limits for the GPU. 1/Module: This
changes the power limits for the module containing multiple components.
E.g. GPU and CPU.</p>
<h2>-cc, --cuda-clocks=MODE</h2>
<p>Overrides or restores default CUDA clocks. Available arguments are:
0\|RESTORE_DEFAULT or 1\|OVERRIDE. Requires root.</p>
<h2>-am, --accounting-mode=MODE</h2>
<p>Enables or disables GPU Accounting. With GPU Accounting one can keep
track of usage of resources throughout lifespan of a single process.
Only on supported devices from Kepler family. Requires administrator
privileges. Available arguments are 0\|DISABLED or 1\|ENABLED.</p>
<h2>-caa, --clear-accounted-apps</h2>
<p>Clears all processes accounted so far. Only on supported devices from
Kepler family. Requires administrator privileges.</p>
<h2>--auto-boost-default=MODE</h2>
<p>This option is deprecated and will be removed in a future CUDA
release. Set the default auto boost policy to 0/DISABLED or 1/ENABLED,
enforcing the change only after the last boost client has exited. Only
on certain Tesla devices from the Kepler+ family and Maxwell-based
GeForce devices. Requires root.</p>
<h2>--auto-boost-permission=MODE</h2>
<p>This option is deprecated and will be removed in a future CUDA
release. Allow non-admin/root control over auto boost mode. Available
arguments are 0\|UNRESTRICTED, 1\|RESTRICTED. Only on certain Tesla
devices from the Kepler+ family and Maxwell-based GeForce devices.
Requires root.</p>
<h2>-mig, --multi-instance-gpu=MODE</h2>
<p>Enables or disables Multi Instance GPU mode. Only supported on
devices based on the NVIDIA Ampere architecture. Requires root.
Available arguments are 0\|DISABLED or 1\|ENABLED.</p>
<h2>-gtt, --gpu-target-temp=MODE</h2>
<p>Set GPU Target Temperature for a GPU in degrees celsius. Target
temperature should be within limits supported by GPU. These limits can
be retrieved by using query option with SUPPORTED_GPU_TARGET_TEMP.
Requires Root.</p>
<h2>--set-hostname=hostname</h2>
<p>Set the hostname associated with device. Should be a maximum length
of 64 characters (including the terminating NULL character). Requires
root.</p>
<h2>--get-hostname</h2>
<p>Retrieves the hostname associated with the device.</p>
<h2>[plus optionally]</h2>
<h2>-i, --id=ID</h2>
<p>Modify a single specified GPU. The specified id may be the GPU/Unit's
0-based index in the natural enumeration returned by the driver, the
GPU's board serial number, the GPU's UUID, or the GPU's PCI bus ID (as
domain:bus:device.function in hex). It is recommended that users
desiring consistency use either UUID or PCI bus ID, since device
enumeration ordering is not guaranteed to be consistent between reboots
and board serial number might be shared between multiple GPUs on the
same board.</p>
<h2>-eom, --error-on-warning</h2>
<p>Return a non-zero error for warnings.</p>
<h2>UNIT MODIFICATION OPTIONS</h2>
<h2>-t, --toggle-led=STATE</h2>
<p>Set the LED indicator state on the front and back of the unit to the
specified color. See the (<em>UNIT ATTRIBUTES</em>) section for a
description of the LED states. Allowed colors are 0\|GREEN and 1\|AMBER.
Requires root.</p>
<h2>[plus optionally]</h2>
<h2>-i, --id=ID</h2>
<p>Modify a single specified Unit. The specified id is the Unit's
0-based index in the natural enumeration returned by the driver.</p>
<h2>SHOW DTD OPTIONS</h2>
<h2>--dtd</h2>
<p>Display Device or Unit DTD.</p>
<h2>[plus optionally]</h2>
<h2>-f FILE, --filename=FILE</h2>
<p>Redirect query output to the specified file in place of the default
stdout. The specified file will be overwritten.</p>
<h2>-u, --unit</h2>
<p>Display Unit DTD instead of device DTD.</p>
<h2>topo</h2>
<p>Display topology information about the system. Use "nvidia-smi topo
-h" for more information. Linux only. Shows all GPUs NVML is able to
detect but CPU and NUMA node affinity information will only be shown for
GPUs with Kepler or newer architectures. Note: GPU enumeration is the
same as NVML.</p>
<h2>drain</h2>
<p>Display and modify the GPU drain states. A drain state is one in
which the GPU is no longer accepting new clients, and is used while
preparing to power down the GPU. Use "nvidia-smi drain -h" for more
information. Linux only.</p>
<h2>nvlink</h2>
<p>Display nvlink information. Use "nvidia-smi nvlink -h" for more
information.</p>
<h2>clocks</h2>
<p>Query and control clocking behavior. Use "nvidia-smi clocks --help"
for more information.</p>
<h2>vgpu</h2>
<p>Display information on GRID virtual GPUs. Use "nvidia-smi vgpu -h"
for more information.</p>
<h2>mig</h2>
<p>Provides controls for MIG management. "nvidia-smi mig -h" for more
information.</p>
<h2>boost-slider</h2>
<p>Provides controls for boost sliders management. "nvidia-smi
boost-slider -h" for more information.</p>
<h2>power-hint</h2>
<p>Provides queries for power hint. "nvidia-smi power-hint -h" for more
information.</p>
<h2>conf-compute</h2>
<p>Provides control and queries for confidential compute. "nvidia-smi
conf-compute -h" for more information.</p>
<h2>power-smoothing</h2>
<p>Provides controls and information for power smoothing. "nvidia-smi
power-smoothing -h" for more information.</p>
<h2>power-profiles</h2>
<p>Profiles controls and information for workload power profiles.
"nvidia-smi power-profiles -h" for more information.</p>
<h2>encodersessions</h2>
<p>Display Encoder Sessions information. "nvidia-smi encodersessions -h"
for more information.</p>
<h1>RETURN VALUE</h1>
<p>Return code reflects whether the operation succeeded or failed and
what was the reason of failure.</p>
<ul>
<li><p>Return code 0 - Success</p></li>
</ul>
<ul>
<li><p>Return code 2 - A supplied argument or flag is invalid</p></li>
</ul>
<ul>
<li><p>Return code 3 - The requested operation is not available on
target device</p></li>
</ul>
<ul>
<li><p>Return code 4 - The current user does not have permission to
access this device or perform this operation</p></li>
</ul>
<ul>
<li><p>Return code 6 - A query to find an object was
unsuccessful</p></li>
</ul>
<ul>
<li><p>Return code 8 - A device's external power cables are not properly
attached</p></li>
</ul>
<ul>
<li><p>Return code 9 - NVIDIA driver is not loaded</p></li>
</ul>
<ul>
<li><p>Return code 10 - NVIDIA Kernel detected an interrupt issue with a
GPU</p></li>
</ul>
<ul>
<li><p>Return code 12 - NVML Shared Library couldn't be found or
loaded</p></li>
</ul>
<ul>
<li><p>Return code 13 - Local version of NVML doesn't implement this
function</p></li>
</ul>
<ul>
<li><p>Return code 14 - infoROM is corrupted</p></li>
</ul>
<ul>
<li><p>Return code 15 - The GPU has fallen off the bus or has otherwise
become inaccessible</p></li>
</ul>
<ul>
<li><p>Return code 255 - Other error or internal driver error
occurred</p></li>
</ul>
<h1>GPU ATTRIBUTES</h1>
<p>The following list describes all possible data returned by the
<strong>-q</strong> device query option. Unless otherwise noted all
numerical results are base 10 and unitless.</p>
<h2>Timestamp</h2>
<p>The current system timestamp at the time nvidia-smi was invoked.
Format is "Day-of-week Month Day HH:MM:SS Year".</p>
<h2>Driver Version</h2>
<p>The version of the installed NVIDIA display driver. This is an
alphanumeric string.</p>
<h2>CUDA Version</h2>
<p>The latest CUDA version supported by the driver. This is usually, but
not always, the version of the CUDA toolkit installed on the system.
This is an alphanumeric string.</p>
<h2>Attached GPUs</h2>
<p>The number of NVIDIA GPUs in the system.</p>
<h2>Product Name</h2>
<p>The official product name of the GPU. This is an alphanumeric string.
For all products.</p>
<h2>Product Brand</h2>
<p>The official brand of the GPU. This is an alphanumeric string. For
all products.</p>
<h2>Product Architecture</h2>
<p>The official architecture name of the GPU. This is an alphanumeric
string. For all products.</p>
<h2>Display Mode</h2>
<p>This field is deprecated, and will be removed in a future
release.</p>
<h2>Display Attached</h2>
<p>A flag that indicates whether a physical display (e.g. monitor) is
currently connected to any of the GPU's connectors. "Yes" indicates an
attached display. "No" indicates otherwise.</p>
<h2>Display Active</h2>
<p>A flag that indicates whether a display is initialized on the GPU's
(e.g. memory is allocated on the device for display). Display can be
active even when no monitor is physically attached. "Enabled" indicates
an active display. "Disabled" indicates otherwise.</p>
<h2>Persistence Mode</h2>
<p>A flag that indicates whether persistence mode is enabled for the
GPU. Value is either "Enabled" or "Disabled". When persistence mode is
enabled the NVIDIA driver remains loaded even when no active clients,
such as X11 or nvidia-smi, exist. This minimizes the driver load latency
associated with running dependent apps, such as CUDA programs. For all
CUDA-capable products. Linux only.</p>
<h2>Addressing Mode</h2>
<p>A field that indicates which addressing mode is currently active. The
value is "ATS" or "HMM" or "None". When the mode is "ATS", system
allocated memory like malloc is addressable from the GPU via Address
Translation Services. This means there is effectively a single set of
page tables used by both the CPU and the GPU. When the mode is "HMM",
system allocated memory like malloc is addressable from the GPU via
software-based mirroring of the CPU's page tables, on the GPU. When the
mode is "None", neither ATS nor HMM is active. Linux only.</p>
<h2>MIG Mode</h2>
<p>MIG Mode configuration status</p>
<dl>
<dt><strong>Current</strong></dt>
<dd>
<p>MIG mode currently in use - NA/Enabled/Disabled</p>
</dd>
</dl>
<dl>
<dt><strong>Pending</strong></dt>
<dd>
<p>Pending configuration of MIG Mode - Enabled/Disabled</p>
</dd>
</dl>
<h2>MIG Device</h2>
<p>When MIG is enabled, each MIG device has the following attributes
displayed:</p>
<dl>
<dt><strong>Index</strong></dt>
<dd>
<p>Unique identifier for this MIG device within its parent GPU.</p>
</dd>
</dl>
<dl>
<dt><strong>GPU Instance ID</strong></dt>
<dd>
<p>Identifier of the GPU instance that this MIG device belongs to.</p>
</dd>
</dl>
<dl>
<dt><strong>Compute Instance ID</strong></dt>
<dd>
<p>Identifier of the compute instance within the GPU instance.</p>
</dd>
</dl>
<dl>
<dt><strong>Device Attributes</strong></dt>
<dd>
<p>Hardware engines allocated to this MIG device. These are shared among
compute instances associated with the same GPU instance.</p>
</dd>
</dl>
<blockquote>
<p><strong>Multiprocessor count</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Number of SMs (Streaming Multiprocessors)</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Copy Engine count</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Number of copy engines</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Encoder count</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Number of video encoders</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Decoder count</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Number of video decoders</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>OFA count</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Number of OFAs (Optical Flow Accelerators)</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>JPG count</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Number of JPEG encoders/decoders</p>
</blockquote>
</blockquote>
<dl>
<dt><strong>ECC Errors</strong></dt>
<dd>
<p>ECC error counts for this MIG device.</p>
</dd>
</dl>
<blockquote>
<p><strong>SRAM uncorrectable errors</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Number of uncorrectable errors detected in any of the SRAMs.</p>
</blockquote>
</blockquote>
<dl>
<dt><strong>Shared FB Memory Usage</strong></dt>
<dd>
<p>FB memory allocation and usage of this MIG device. This is shared
among the compute instances associated with the same GPU instance.</p>
</dd>
</dl>
<blockquote>
<p><strong>Total</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Total size of FB memory.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Reserved</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Reserved size of FB memory.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Used</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Used size of FB memory.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Free</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Available size of FB memory.</p>
</blockquote>
</blockquote>
<dl>
<dt><strong>Shared BAR1 Memory</strong></dt>
<dd>
<p>BAR1 memory allocation and usage of this MIG device. This is shared
among the compute instances associated with the same GPU instance.</p>
</dd>
</dl>
<blockquote>
<p><strong>Total</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Total size of BAR1 memory.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Used</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Used size of BAR1 memory.</p>
</blockquote>
</blockquote>
<blockquote>
<p><strong>Free</strong></p>
</blockquote>
<blockquote>
<blockquote>
<p>Available size of BAR1 memory.</p>
</blockquote>
</blockquote>
<h2>Accounting Mode</h2>
<p>A flag that indicates whether accounting mode is enabled for the GPU.
Value is either "Enabled" or "Disabled". When accounting is enabled
statistics are calculated for each compute process running on the GPU.
Statistics can be queried during the lifetime or after termination of
the process. The execution time of process is reported as 0 while the
process is in running state and updated to actual execution time after
the process has terminated. See --help-query-accounted-apps for more
info.</p>
<h2>Accounting Mode Buffer Size</h2>
<p>Returns the size of the circular buffer that holds list of processes
that can be queried for accounting stats. This is the maximum number of
processes that accounting information will be stored for before
information about oldest processes will get overwritten by information
about new processes.</p>
<h2>Driver Model</h2>
<p>On Windows, the TCC and WDDM driver models are supported. The driver
model can be changed with the (-dm) or (-fdm) flags. The TCC driver
model is optimized for compute applications. I.E. kernel launch times
will be quicker with TCC. The WDDM driver model is designed for graphics
applications and is not recommended for compute applications. Linux does
not support multiple driver models, and will always have the value of
"N/A".</p>
<dl>
<dt><strong>Current</strong></dt>
<dd>
<p>The driver model currently in use. Always "N/A" on Linux.</p>
</dd>
</dl>
<dl>
<dt><strong>Pending</strong></dt>
<dd>
<p>The driver model that will be used on the next reboot. Always "N/A"
on Linux.</p>
</dd>
</dl>
<h2>Serial Number</h2>
<p>This number matches the serial number physically printed on each
board. It is a globally unique immutable alphanumeric value.</p>
<h2>GPU UUID</h2>
<p>This value is the globally unique immutable alphanumeric identifier
of the GPU. It does not correspond to any physical label on the
board.</p>
<h2>GPU PDI</h2>
<p>This value is the Per Device Identifier of the GPU. It is a 64-bit
value that provides uniqueness guarantee for the GPU.</p>
<h2>Minor Number</h2>
<p>The minor number for the device is such that the NVIDIA device node
file for each GPU will have the form /dev/nvidia[minor number].
Available only on Linux platform.</p>
<h2>VBIOS Version</h2>
<p>The BIOS of the GPU board.</p>
<h2>MultiGPU Board</h2>
<p>Whether or not this GPU is part of a multiGPU board.</p>
<h2>Board ID</h2>
<p>The unique board ID assigned by the driver. If two or more GPUs have
the same board ID and the above "MultiGPU" field is true then the GPUs
are on the same board.</p>
<h2>Board Part Number</h2>
<p>The unique part number of the GPU's board</p>
<h2>GPU Part Number</h2>
<p>The unique part number of the GPU</p>
<h2>FRU Part Number</h2>
<p>Unique FRU part number of the GPU</p>
<h2>Platform Info</h2>
<p>Platform Information are compute tray platform specific information.
They are GPU's positional index and platform identifying
information.</p>
<p><strong>Chassis Serial Number</strong></p>
<p>Serial Number of the chassis containing this GPU.</p>
<p><strong>Slot Number</strong></p>
<p>The slot number in the chassis containing this GPU (includes
switches).</p>
<p><strong>Tray Index</strong></p>
<p>The tray index within the compute slots in the chassis containing
this GPU (does not include switches).</p>
<p><strong>Host ID</strong></p>
<p>Index of the node within the slot containing this GPU.</p>
<p><strong>Peer Type</strong></p>
<p>Platform indicated NVLink-peer type (e.g. switch present or not).</p>
<p><strong>Module Id</strong></p>
<p>ID of this GPU within the node.</p>
<p><strong>GPU Fabric GUID</strong></p>
<p>Fabric ID for this GPU.</p>
<h2>Inforom Version</h2>
<p>Version numbers for each object in the GPU board's inforom storage.
The inforom is a small, persistent store of configuration and state data
for the GPU. All inforom version fields are numerical. It can be useful
to know these version numbers because some GPU features are only
available with inforoms of a certain version or higher.</p>
<p>If any of the fields below return Unknown Error additional Inforom
verification check is performed and appropriate warning message is
displayed.</p>
<dl>
<dt><strong>Image Version</strong></dt>
<dd>
<p>Global version of the infoROM image. Image version just like VBIOS
version uniquely describes the exact version of the infoROM flashed on
the board in contrast to infoROM object version which is only an
indicator of supported features.</p>
</dd>
</dl>
<dl>
<dt><strong>OEM Object</strong></dt>
<dd>
<p>Version for the OEM configuration data.</p>
</dd>
</dl>
<dl>
<dt><strong>ECC Object</strong></dt>
<dd>
<p>Version for the ECC recording data.</p>
</dd>
</dl>
<dl>
<dt><strong>Power Management Object</strong></dt>
<dd>
<p>Version for the power management data.</p>
</dd>
</dl>
<dl>
<dt><strong>Inforom checksum validation</strong></dt>
<dd>
<p>Inforom checksum validation ("valid", "invalid", "N/A") Only
available via --query-gpu inforom.checksum_validation</p>
</dd>
</dl>
<h2>Inforom BBX Object Flush</h2>
<p>Information about flushing of the blackbox data to the inforom
storage.</p>
<dl>
<dt><strong>Latest Timestamp</strong></dt>
<dd>
<p>The timestamp of the latest flush of the BBX Object during the
current run.</p>
</dd>
</dl>
<dl>
<dt><strong>Latest Duration</strong></dt>
<dd>
<p>The duration of the latest flush of the BBX Object during the current
run.</p>
</dd>
</dl>
<h2>GPU Operation Mode</h2>
<p>GOM allows one to reduce power usage and optimize GPU throughput by
disabling GPU features.</p>
<p>Each GOM is designed to meet specific user needs.</p>
<p>In "All On" mode everything is enabled and running at full speed.</p>
<p>The "Compute" mode is designed for running only compute tasks.
Graphics operations are not allowed.</p>
<p>The "Low Double Precision" mode is designed for running graphics
applications that don't require high bandwidth double precision.</p>
<p>GOM can be changed with the (--gom) flag.</p>
<p>Supported on GK110 M-class and X-class Tesla products from the Kepler
family. Not supported on Quadro and Tesla C-class products. Low Double
Precision and All On modes are the only modes available for supported
GeForce Titan products.</p>
<dl>
<dt><strong>Current</strong></dt>
<dd>
<p>The GOM currently in use.</p>
</dd>
</dl>
<dl>
<dt><strong>Pending</strong></dt>
<dd>
<p>The GOM that will be used on the next reboot.</p>
</dd>
</dl>
<h2>GPU C2C Mode</h2>
<p>The C2C mode of the GPU.</p>
<h2>GPU Reset Status</h2>
<p>Reset status of the GPU. This functionality is deprecated.</p>
<dl>
<dt><strong>Reset Required</strong></dt>
<dd>
<p>Requested functionality has been deprecated</p>
</dd>
</dl>
<dl>
<dt><strong>Drain and Reset Recommended</strong></dt>
<dd>
<p>Requested functionality has been deprecated</p>
</dd>
</dl>
<h2>GPU Recovery Action</h2>
<p>Action to take to clear fault that previously happened. It is not
intended for determining which fault triggered recovery action.<br />
Possible values: None, Reset, Reboot, Drain P2P, Drain and Reset</p>
<p><strong>None</strong></p>
<p>No recovery action needed</p>
<p><strong>Reset</strong></p>
<p>Example scenario - Uncontained HBM/SRAM UCE<br />
The GPU has encountered a fault that requires a reset to recover.<br />
Terminate all GPU processes, reset the GPU using 'nvidia-smi -r', and
the GPU can be used again by starting new GPU processes.</p>
<p><strong>Reboot</strong></p>
<p>Example scenario - UVM fatal error<br />
The GPU has encountered a fault may have left the OS in an inconsistent
state.<br />
Reboot the operating system to restore the OS back to a consistent
state.<br />
Node reboot required.<br />
Application cannot restart without node reboot<br />
OS warm reboot is sufficient (no need for AC/DC cycle)</p>
<p><strong>Drain P2P</strong></p>
<p>Example scenario - N/A<br />
The GPU has encountered a fault that requires all peer-to-peer traffic
to be quiesced.<br />
Terminate all GPU processes that conduct peer-to-peer traffic and
disable UVM persistence mode.<br />
Disable job scheduling (no new jobs), stop all applications when
convenient, if persistence mode is enabled, disable it<br />
Once all peer-to-peer traffic are drained, query
NVML_FI_DEV_GET_GPU_RECOVERY_ACTION again, which will return one of the
other actions.<br />
If still DRAIN_P2P, then GPU reset.</p>
<p><strong>Drain and Reset</strong></p>
<p>Example scenario - Contained HBM UCE<br />
Reset Recommended.<br />
The GPU has encountered a fault that results the GPU to temporarily
operate at a reduced capacity, such as part of its frame buffer memory
being offlined, or some of its MIG partitions down.<br />
No new work should be scheduled on the GPU, but existing work that
didn't get affected are safe to continue until they finish or reach a
good checkpoint.<br />
Safe to restart application (memory capacity will be reduced due to
dynamic page offlining), but need to eventually reset (to get row
remap).<br />
Asserted only for UCE row remaps.<br />
After all existing work have drained, reset the GPU to regain its full
capacity.</p>
<h2>GSP Firmware Version</h2>
<p>Firmware version of GSP. This is an alphanumeric string.</p>
<h2>PCI</h2>
<p>Basic PCI info for the device. Some of this information may change
whenever cards are added/removed/moved in a system. For all
products.</p>
<dl>
<dt><strong>Bus</strong></dt>
<dd>
<p>PCI bus number, in hex</p>
</dd>
</dl>
<dl>
<dt><strong>Device</strong></dt>
<dd>
<p>PCI device number, in hex</p>
</dd>
</dl>
<dl>
<dt><strong>Domain</strong></dt>
<dd>
<p>PCI domain number, in hex</p>
</dd>
</dl>
<dl>
<dt><strong>Base Classcode</strong></dt>
<dd>
<p>PCI Base classcode, in hex</p>
</dd>
</dl>
<dl>
<dt><strong>Sub Classcode</strong></dt>
<dd>
<p>PCI Sub classcode, in hex</p>
</dd>
</dl>
<dl>
<dt><strong>Device Id</strong></dt>
<dd>
<p>PCI vendor device id, in hex</p>
</dd>
</dl>
<dl>
<dt><strong>Sub System Id</strong></dt>
<dd>
<p>PCI Sub System id, in hex</p>
</dd>
</dl>
<dl>
<dt><strong>Bus Id</strong></dt>
<dd>
<p>PCI bus id as "domain:bus:device.function", in hex</p>
</dd>
</dl>
<h2>GPU Link information</h2>
<p>The PCIe link generation and bus width</p>
<dl>
<dt><strong>Current</strong></dt>
<dd>
<p>The current link generation and width. These may be reduced when the
GPU is not in use.</p>
</dd>
</dl>
<dl>
<dt><strong>Max</strong></dt>
<dd>
<p>The maximum link generation and width possible with this GPU and
system configuration. For example, if the GPU supports a higher PCIe
generation than the system supports then this reports the system PCIe
generation.</p>
</dd>
</dl>
<h2>Bridge Chip</h2>
<p>Information related to Bridge Chip on the device. The bridge chip
firmware is only present on certain boards and may display "N/A" for
some newer multiGPUs boards.</p>
<dl>
<dt><strong>Type</strong></dt>
<dd>
<p>The type of bridge chip. Reported as N/A if doesn't exist.</p>
</dd>
</dl>
<dl>
<dt><strong>Firmware Version</strong></dt>
<dd>
<p>The firmware version of the bridge chip. Reported as N/A if doesn't
exist.</p>
</dd>
</dl>
<h2>Replays Since Reset</h2>
<p>The number of PCIe replays since reset.</p>
<h2>Replay Number Rollovers</h2>
<p>The number of PCIe replay number rollovers since reset. A replay
number rollover occurs after 4 consecutive replays and results in
retraining the link.</p>
<h2>Tx Throughput</h2>
<p>The GPU-centric transmission throughput across the PCIe bus in MB/s
over the past 20ms. Only supported on Maxwell architectures and
newer.</p>
<h2>Rx Throughput</h2>
<p>The GPU-centric receive throughput across the PCIe bus in MB/s over
the past 20ms. Only supported on Maxwell architectures and newer.</p>
<h2>Atomic Caps</h2>
<p>The PCIe atomic capabilities of outbound/inbound operations of the
GPU.</p>
<h2>Fan Speed</h2>
<p>The fan speed value is the percent of the product's maximum noise
tolerance fan speed that the device's fan is currently intended to run
at. This value may exceed 100% in certain cases. Note: The reported
speed is the intended fan speed. If the fan is physically blocked and
unable to spin, this output will not match the actual fan speed. Many
parts do not report fan speeds because they rely on cooling via fans in
the surrounding enclosure. For all discrete products with dedicated
fans.</p>
<h2>Performance State</h2>
<p>The current performance state for the GPU. States range from P0
(maximum performance) to P12 (minimum performance).</p>
<h2>Clocks Event Reasons</h2>
<p>Retrieves information about factors that are reducing the frequency
of clocks.</p>
<p>If all event reasons are returned as "Not Active" it means that
clocks are running as high as possible.</p>
<dl>
<dt><strong>Idle</strong></dt>
<dd>
<p>This option is deprecated and will be removed in a future CUDA
release. Nothing is running on the GPU and the clocks are dropping to
Idle state.</p>
</dd>
</dl>
<dl>
<dt><strong>Application Clocks Setting</strong></dt>
<dd>
<p>This option is deprecated and will be removed in a future CUDA
release. GPU clocks are limited by applications clocks setting. E.g. can
be changed using nvidia-smi --applications-clocks=&lt;Desired Clock Freq
in MHz&gt;</p>
</dd>
</dl>
<dl>
<dt><strong>SW Power Cap</strong></dt>
<dd>
<p>SW Power Scaling algorithm is reducing the clocks below requested
clocks because the GPU is consuming too much power. E.g. SW power cap
limit can be changed with nvidia-smi --power-limit=&lt;Power Limit Value
in W&gt;</p>
</dd>
</dl>
<dl>
<dt><strong>HW Slowdown</strong></dt>
<dd>
<p>This option will be removed a future CUDA release. HW Slowdown is
engaged, reducing the core clocks by a factor of 2 or more. It is active
if either HW Thermal Slowdown or HW Power Brake are active.</p>
</dd>
</dl>
<dl>
<dt><strong>HW Thermal Slowdown</strong></dt>
<dd>
<p>HW Thermal Slowdowns are reducing the core clocks by a factor of 2 or
more due to temperature being too high.</p>
</dd>
</dl>
<dl>
<dt><strong>HW Power Brake</strong></dt>
<dd>
<p>External Power Brake Assertion is triggered (e.g. by the system power
supply).</p>
</dd>
</dl>
<dl>
<dt><strong>Sync Boost</strong></dt>
<dd>
<p>This GPU has been added to a Sync boost group with nvidia-smi or DCGM
in order to maximize performance per watt. All GPUs will be limited by
the frequency which can be achieved by the slowest GPU. Look at the
throttle reasons for other GPUs in the system to see why those GPUs are
holding this one at lower clocks.</p>
</dd>
</dl>
<dl>
<dt><strong>SW Thermal Slowdown</strong></dt>
<dd>
<p>SW Thermal capping algorithm is reducing clocks below requested
clocks because GPU temperature is higher than Max Operating Temp</p>
</dd>
</dl>
<dl>
<dt><strong>Display Clock Setting</strong></dt>
<dd>
<p>This field will be removed in a future CUDA release. GPU clocks are
limited by current setting of Display clocks. Only supported on Volta
devices.</p>
</dd>
</dl>
<h2>Clock Event Reasons Counters</h2>
<p>Counters, in microseconds, for the amount of time factors have been
reducing the frequency of clocks.</p>
<dl>
<dt><strong>SW Power Capping</strong></dt>
<dd>
<p>Amount of time SW Power Scaling algorithm has reduced the clocks
below requested clocks because the GPU was consuming too much power.</p>
</dd>
</dl>
<dl>
<dt><strong>Sync Boost Group</strong></dt>
<dd>
<p>Amount of time the clock frequency of this GPU was reduced to match
the minimum possible clock across the sync boost group.</p>
</dd>
</dl>
<dl>
<dt><strong>SW Thermal Slowdown</strong></dt>
<dd>
<p>Amount of time SW Thermal capping algorithm has reduced clocks below
requested clocks because GPU temperature was higher than Max Operating
Temp.</p>
</dd>
</dl>
<dl>
<dt><strong>HW Thermal Slowdown</strong></dt>
<dd>
<p>Amount of time HW Thermal Slowdown was engaged, reducing the core
clocks by a factor of 2 or more, due to temperature being too high.</p>
</dd>
</dl>
<dl>
<dt><strong>HW Power Braking</strong></dt>
<dd>
<p>Amount of time External Power Brake Assertion was triggered (e.g. by
the system power supply).</p>
</dd>
</dl>
<h2>Sparse Operation Mode</h2>
<p>A flag that indicates whether sparse operation mode is enabled for
the GPU. Value is either "Enabled" or "Disabled". Reported as "N/A" if
not supported.</p>
<h2>FB Memory Usage</h2>
<p>On-board frame buffer memory information. Reported total memory can
be affected by ECC state. If ECC does affect the total available memory,
memory is decreased by several percent, due to the requisite parity
bits. The driver may also reserve a small amount of memory for internal
use, even without active work on the GPU. On systems where GPUs are NUMA
nodes, the accuracy of FB memory utilization provided by nvidia-smi
depends on the memory accounting of the operating system. This is
because FB memory is managed by the operating system instead of the
NVIDIA GPU driver. Typically, pages allocated from FB memory are not
released even after the process terminates to enhance performance. In
scenarios where the operating system is under memory pressure, it may
resort to utilizing FB memory. Such actions can result in discrepancies
in the accuracy of memory reporting. For all products.</p>
<dl>
<dt><strong>Total</strong></dt>
<dd>
<p>Total size of FB memory.</p>
</dd>
</dl>
<dl>
<dt><strong>Reserved</strong></dt>
<dd>
<p>Reserved size of FB memory.</p>
</dd>
</dl>
<dl>
<dt><strong>Used</strong></dt>
<dd>
<p>Used size of FB memory.</p>
</dd>
</dl>
<dl>
<dt><strong>Free</strong></dt>
<dd>
<p>Available size of FB memory.</p>
</dd>
</dl>
<h2>BAR1 Memory Usage</h2>
<p>BAR1 is used to map the FB (device memory) so that it can be directly
accessed by the CPU or by 3rd party devices (peer-to-peer on the PCIe
bus).</p>
<dl>
<dt><strong>Total</strong></dt>
<dd>
<p>Total size of BAR1 memory.</p>
</dd>
</dl>
<dl>
<dt><strong>Used</strong></dt>
<dd>
<p>Used size of BAR1 memory.</p>
</dd>
</dl>
<dl>
<dt><strong>Free</strong></dt>
<dd>
<p>Available size of BAR1 memory.</p>
</dd>
</dl>
<h2>Compute Mode</h2>
<p>The compute mode flag indicates whether individual or multiple
compute applications may run on the GPU.</p>
<p>"Default" means multiple contexts are allowed per device.</p>
<p>"Exclusive Process" means only one context is allowed per device,
usable from multiple threads at a time.</p>
<p>"Prohibited" means no contexts are allowed per device (no compute
apps).</p>
<p>"EXCLUSIVE_PROCESS" was added in CUDA 4.0. Prior CUDA releases
supported only one exclusive mode, which is equivalent to
"EXCLUSIVE_THREAD" in CUDA 4.0 and beyond.</p>
<p>For all CUDA-capable products.</p>
<h2>Utilization</h2>
<p>Utilization rates report how busy each GPU is over time, and can be
used to determine how much an application is using the GPUs in the
system. Note: On MIG-enabled GPUs, querying the utilization of encoder,
decoder, jpeg, ofa, gpu, and memory is not currently supported.</p>
<p>Note: During driver initialization when ECC is enabled one can see
high GPU and Memory Utilization readings. This is caused by ECC Memory
Scrubbing mechanism that is performed during driver initialization.</p>
<dl>
<dt><strong>GPU</strong></dt>
<dd>
<p>Percent of time over the past sample period during which one or more
kernels was executing on the GPU. The sample period may be between 1
second and 1/6 second depending on the product.</p>
</dd>
</dl>
<dl>
<dt><strong>Memory</strong></dt>
<dd>
<p>Percent of time over the past sample period during which global
(device) memory was being read or written. The sample period may be
between 1 second and 1/6 second depending on the product.</p>
</dd>
</dl>
<dl>
<dt><strong>Encoder</strong></dt>
<dd>
<p>Percent of time over the past sample period during which the GPU's
video encoder was being used. The sampling rate is variable and can be
obtained directly via the nvmlDeviceGetEncoderUtilization() API</p>
</dd>
</dl>
<dl>
<dt><strong>Decoder</strong></dt>
<dd>
<p>Percent of time over the past sample period during which the GPU's
video decoder was being used. The sampling rate is variable and can be
obtained directly via the nvmlDeviceGetDecoderUtilization() API</p>
</dd>
</dl>
<dl>
<dt><strong>JPEG</strong></dt>
<dd>
<p>Percent of time over the past sample period during which the GPU's
JPEG decoder was being used. The sampling rate is variable and can be
obtained directly via the nvmlDeviceGetJpgUtilization() API</p>
</dd>
</dl>
<dl>
<dt><strong>OFA</strong></dt>
<dd>
<p>Percent of time over the past sample period during which the GPU's
OFA (Optical Flow Accelerator) was being used. The sampling rate is
variable and can be obtained directly via the
nvmlDeviceGetOfaUtilization() API</p>
</dd>
</dl>
<h2>Encoder Stats</h2>
<p>Encoder Stats report the count of active encoder sessions, along with
the average Frames Per Second (FPS) and average latency (in
microseconds) for all these active sessions on this device.</p>
<dl>
<dt><strong>Active Sessions</strong></dt>
<dd>
<p>The total number of active encoder sessions on this device.</p>
</dd>
</dl>
<dl>
<dt><strong>Average FPS</strong></dt>
<dd>
<p>The average Frame Per Sencond (FSP) of all active encoder sessions on
this device.</p>
</dd>
</dl>
<dl>
<dt><strong>Average Latency</strong></dt>
<dd>
<p>The average latency in microseconds of all active encoder sessions on
this device.</p>
</dd>
</dl>
<h2>DRAM Encryption Mode</h2>
<p>A flag that indicates whether DRAM Encryption support is enabled. May
be either "Enabled" or "Disabled". Changes to DRAM Encryption mode
require a reboot. Requires Inforom ECC object.</p>
<dl>
<dt><strong>Current</strong></dt>
<dd>
<p>The DRAM Encryption mode that the GPU is currently operating
under.</p>
</dd>
</dl>
<dl>
<dt><strong>Pending</strong></dt>
<dd>
<p>The DRAM Encryption mode that the GPU will operate under after the
next reboot.</p>
</dd>
</dl>
<h2>ECC Mode</h2>
<p>A flag that indicates whether ECC support is enabled. May be either
"Enabled" or "Disabled". Changes to ECC mode require a reboot. Requires
Inforom ECC object version 1.0 or higher.</p>
<dl>
<dt><strong>Current</strong></dt>
<dd>
<p>The ECC mode that the GPU is currently operating under.</p>
</dd>
</dl>
<dl>
<dt><strong>Pending</strong></dt>
<dd>
<p>The ECC mode that the GPU will operate under after the next
reboot.</p>
</dd>
</dl>
<h2>ECC Errors</h2>
<p>NVIDIA GPUs can provide error counts for various types of ECC errors.
Some ECC errors are either single or double bit, where single bit errors
are corrected and double bit errors are uncorrectable. Texture memory
errors may be correctable via resend or uncorrectable if the resend
fails. These errors are available across two timescales (volatile and
aggregate). Single bit ECC errors are automatically corrected by the HW
and do not result in data corruption. Double bit errors are detected but
not corrected. Please see the ECC documents on the web for information
on compute application behavior when double bit errors occur. Volatile
error counters track the number of errors detected since the last driver
load. Aggregate error counts persist indefinitely and thus act as a
lifetime counter.</p>
<p>A note about volatile counts: On Windows this is once per boot. On
Linux this can be more frequent. On Linux the driver unloads when no
active clients exist. Hence, if persistence mode is enabled or there is
always a driver client active (e.g. X11), then Linux also sees per-boot
behavior. If not, volatile counts are reset each time a compute app is
run.</p>
<p>Tesla and Quadro products pre-volta can display total ECC error
counts, as well as a breakdown of errors based on location on the chip.
The locations are described below. Location-based data for aggregate
error counts requires Inforom ECC object version 2.0. All other ECC
counts require ECC object version 1.0.</p>
<dl>
<dt><strong>Device Memory</strong></dt>
<dd>
<p>Errors detected in global device memory.</p>
</dd>
</dl>
<dl>
<dt><strong>Register File</strong></dt>
<dd>
<p>Errors detected in register file memory.</p>
</dd>
</dl>
<dl>
<dt><strong>L1 Cache</strong></dt>
<dd>
<p>Errors detected in the L1 cache.</p>
</dd>
</dl>
<dl>
<dt><strong>L2 Cache</strong></dt>
<dd>
<p>Errors detected in the L2 cache.</p>
</dd>
</dl>
<dl>
<dt><strong>Texture Memory</strong></dt>
<dd>
<p>Parity errors detected in texture memory.</p>
</dd>
</dl>
<dl>
<dt><strong>Total</strong></dt>
<dd>
<p>Total errors detected across entire chip. Sum of <strong>Device
Memory</strong>, <strong>Register File</strong>, <strong>L1
Cache</strong>, <strong>L2 Cache</strong> and <strong>Texture
Memory</strong>.</p>
</dd>
</dl>
<p>On Turing the output is such:</p>
<dl>
<dt><strong>SRAM Correctable</strong></dt>
<dd>
<p>Number of correctable errors detected in any of the SRAMs</p>
</dd>
</dl>
<dl>
<dt><strong>SRAM Uncorrectable</strong></dt>
<dd>
<p>Number of uncorrectable errors detected in any of the SRAMs</p>
</dd>
</dl>
<dl>
<dt><strong>DRAM Correctable</strong></dt>
<dd>
<p>Number of correctable errors detected in the DRAM</p>
</dd>
</dl>
<dl>
<dt><strong>DRAM Uncorrectable</strong></dt>
<dd>
<p>Number of uncorrectable errors detected in the DRAM</p>
</dd>
</dl>
<p>On Ampere+ The categorization of SRAM errors has been expanded upon.
SRAM errors are now categorized as either parity or SEC-DED (single
error correctable/double error detectable) depending on which unit hit
the error. A histogram has been added that categorizes what unit hit the
SRAM error. Additionally a flag has been added that indicates if the
threshold for the specific SRAM has been exceeded.</p>
<dl>
<dt><strong>SRAM Uncorrectable Parity</strong></dt>
<dd>
<p>Number of uncorrectable errors detected in SRAMs that are parity
protected</p>
</dd>
</dl>
<dl>
<dt><strong>SRAM Uncorrectable SEC-DED</strong></dt>
<dd>
<p>Number of uncorrectable errors detected in SRAMs that are SEC-DED
protected</p>
</dd>
</dl>
<dl>
<dt><strong>Aggregate Uncorrectable SRAM Sources</strong></dt>
<dd>
<p>Details about the sources of Aggregate uncorrectable SRAM errors</p>
</dd>
</dl>
<dl>
<dt><strong>SRAM L2</strong></dt>
<dd>
<p>Errors that occurred in the L2 cache</p>
</dd>
</dl>
<dl>
<dt><strong>SRAM SM</strong></dt>
<dd>
<p>Errors that occurred in the SM</p>
</dd>
</dl>
<dl>
<dt><strong>SRAM Microcontroller</strong></dt>
<dd>
<p>Errors that occurred in a microcontroller (PMU/GSP etc...)</p>
</dd>
</dl>
<dl>
<dt><strong>SRAM PCIE</strong></dt>
<dd>
<p>Errors that occrred in any PCIE related unit</p>
</dd>
</dl>
<dl>
<dt><strong>SRAM Other</strong></dt>
<dd>
<p>Errors occuring in anything else not covered above</p>
</dd>
</dl>
<p>If one of the repair flags is pending, check the GPU Recovery action
and take the appropriate steps.</p>
<dl>
<dt><strong>Channel Repair Pending</strong></dt>
<dd>
<p>Indicates if a Channel repair is pending</p>
</dd>
</dl>
<dl>
<dt><strong>TPC Repair Pending</strong></dt>
<dd>
<p>Indicates if a TPC repair is pending</p>
</dd>
</dl>
<dl>
<dt><strong>Unrepairable Memory</strong></dt>
<dd>
<p>Indicates if there is unrepairable memory</p>
</dd>
</dl>
<h2>Page Retirement</h2>
<p>NVIDIA GPUs can retire pages of GPU device memory when they become
unreliable. This can happen when multiple single bit ECC errors occur
for the same page, or on a double bit ECC error. When a page is retired,
the NVIDIA driver will hide it such that no driver, or application
memory allocations can access it.</p>
<p><strong>Double Bit ECC</strong> The number of GPU device memory pages
that have been retired due to a double bit ECC error.</p>
<p><strong>Single Bit ECC</strong> The number of GPU device memory pages
that have been retired due to multiple single bit ECC errors.</p>
<p><strong>Pending</strong> Checks if any GPU device memory pages are
pending blacklist on the next reboot. Pages that are retired but not yet
blacklisted can still be allocated, and may cause further reliability
issues.</p>
<h2>Row Remapper</h2>
<p>NVIDIA GPUs can remap rows of GPU device memory when they become
unreliable. This can happen when a single uncorrectable ECC error or
multiple correctable ECC errors occur on the same row. When a row is
remapped, the NVIDIA driver will remap the faulty row to a reserved row.
All future accesses to the row will access the reserved row instead of
the faulty row. This feature is available on Ampere+</p>
<p><strong>Correctable Error</strong> The number of rows that have been
remapped due to correctable ECC errors.</p>
<p><strong>Uncorrectable Error</strong> The number of rows that have
been remapped due to uncorrectable ECC errors.</p>
<p><strong>Pending</strong> Indicates whether or not a row is pending
remapping. The GPU must be reset for the remapping to go into
effect.</p>
<p><strong>Remapping Failure Occurred</strong> Indicates whether or not
a row remapping has failed in the past.</p>
<p><strong>Bank Remap Availability Histogram</strong> Each memory bank
has a fixed number of reserved rows that can be used for row remapping.
The histogram will classify the remap availability of each bank into
Maximum, High, Partial, Low and None. Maximum availability means that
all reserved rows are available for remapping while None means that no
reserved rows are available. Correctable row remappings don't count
towards the availability histogram since row remappings due to
correctable row remappings can be evicted by an uncorrectable row
remapping.</p>
<h2>Temperature</h2>
<p>Readings from temperature sensors on the board. All readings are in
degrees C. Not all products support all reading types. In particular,
products in module form factors that rely on case fans or passive
cooling do not usually provide temperature readings. See below for
restrictions.</p>
<p>T.Limit: The T.Limit sensor measures the current margin in degree
Celsius to the maximum operating temperature. As such it is not an
absolute temperature reading rather a relative measurement.</p>
<p>Not all products support T.Limit sensor readings.</p>
<p>When supported, nvidia-smi reports the current T.Limit temperature as
a signed value that counts down. A T.Limit temperature of 0 C or lower
indicates that the GPU may optimize its clock based on thermal
conditions. Further, when the T.Limit sensor is supported, available
temperature thresholds are also reported relative to T.Limit (see below)
instead of absolute measurements.</p>
<dl>
<dt><strong>GPU Current Temp</strong></dt>
<dd>
<p>Core GPU temperature. For all discrete and S-class products.</p>
</dd>
</dl>
<dl>
<dt><strong>GPU T.Limit Temp</strong></dt>
<dd>
<p>Current margin in degrees Celsius from the maximum GPU operating
temperature.</p>
</dd>
</dl>
<dl>
<dt><strong>GPU Shutdown Temp</strong></dt>
<dd>
<p>The temperature at which a GPU will shutdown.</p>
</dd>
</dl>
<dl>
<dt><strong>GPU Shutdown T.Limit Temp</strong></dt>
<dd>
<p>The T.Limit temperature below which a GPU may shutdown. Since
shutdown can only triggered by the maximum GPU temperature it is
possible for the current T.Limit to be more negative than this
threshold.</p>
</dd>
</dl>
<dl>
<dt><strong>GPU Slowdown Temp</strong></dt>
<dd>
<p>The temperature at which a GPU HW will begin optimizing clocks due to
thermal conditions, in order to cool.</p>
</dd>
</dl>
<dl>
<dt><strong>GPU Slowdown T.Limit Temp</strong></dt>
<dd>
<p>The T.Limit temperature at or below which GPU HW may optimize its
clocks for thermal conditions. Since this clock adjustment can only
triggered by the maximum GPU temperature it is possible for the current
T.Limit to be more negative than this threshold.</p>
</dd>
</dl>
<dl>
<dt><strong>GPU Max Operating Temp</strong></dt>
<dd>
<p>The temperature at which GPU SW will optimize its clock for thermal
conditions.</p>
</dd>
</dl>
<dl>
<dt><strong>GPU Max Operating T.Limit Temp</strong></dt>
<dd>
<p>The T.Limit temperature below which GPU SW will optimize its clock
for thermal conditions.</p>
</dd>
</dl>
<dl>
<dt><strong>Memory Current Temp</strong></dt>
<dd>
<p>Current temperature of GPU memory. Only available on supported
devices.</p>
</dd>
</dl>
<dl>
<dt><strong>Memory Max Operating Temp</strong></dt>
<dd>
<p>The temperature at which GPU SW will optimize its memory clocks for
thermal conditions. Only available on supported devices.</p>
</dd>
</dl>
<h2>GPU Power Readings</h2>
<p>Power readings help to shed light on the current power usage of the
GPU, and the factors that affect that usage. When power management is
enabled the GPU limits power draw under load to fit within a predefined
power envelope by manipulating the current performance state. See below
for limits of availability.</p>
<dl>
<dt><strong>Average Power Draw</strong></dt>
<dd>
<p>The average power draw for the entire board for the last second, in
watts. Only supported on Ampere (except GA100) or newer devices.</p>
</dd>
</dl>
<dl>
<dt><strong>Instantaneous Power Draw</strong></dt>
<dd>
<p>The last measured power draw for the entire board, in watts.</p>
</dd>
</dl>
<dl>
<dt><strong>Requested Power Limit</strong></dt>
<dd>
<p>The power limit requested by software, in watts. Set by software such
as nvidia-smi. Power Limit can be adjusted using -pl,--power-limit=
switches.</p>
</dd>
</dl>
<dl>
<dt><strong>Enforced Power Limit</strong></dt>
<dd>
<p>The power management algorithm's power ceiling, in watts. Total board
power draw is manipulated by the power management algorithm such that it
stays under this value. This limit is the minimum of various limits such
as the software limit listed above.</p>
</dd>
</dl>
<dl>
<dt><strong>Default Power Limit</strong></dt>
<dd>
<p>The default power management algorithm's power ceiling, in watts.
Power Limit will be set back to Default Power Limit after driver
unload.</p>
</dd>
</dl>
<dl>
<dt><strong>Min Power Limit</strong></dt>
<dd>
<p>The minimum value in watts that power limit can be set to.</p>
</dd>
</dl>
<dl>
<dt><strong>Max Power Limit</strong></dt>
<dd>
<p>The maximum value in watts that power limit can be set to.</p>
</dd>
</dl>
<h2>Module Power Readings</h2>
<p>Power readings help to shed light on the current power usage of the
Module, and the factors that affect that usage. A module is GPU +
supported NVIDIA CPU + other components which consume power. When power
management is enabled, the Module limits power draw under load to fit
within a predefined power envelope by manipulating the current
performance state. Supported on Hopper and newer datacenter
products.</p>
<dl>
<dt><strong>Average Power Draw</strong></dt>
<dd>
<p>The average power draw for the entire module for the last second, in
watts.</p>
</dd>
</dl>
<dl>
<dt><strong>Instantaneous Power Draw</strong></dt>
<dd>
<p>The last measured power draw for the entire module, in watts.</p>
</dd>
</dl>
<dl>
<dt><strong>Requested Power Limit</strong></dt>
<dd>
<p>The power limit requested by software, in watts, for the whole
module. Set by software such as nvidia-smi. Power Limit can be adjusted
using -pl,--power-limit= switches with -s/--scope=1.</p>
</dd>
</dl>
<dl>
<dt><strong>Enforced Power Limit</strong></dt>
<dd>
<p>The power management algorithm's power ceiling, in watts. Total
module power draw is manipulated by the power management algorithm such
that it stays under this value. This limit is the minimum of various
limits such as the software limit listed above.</p>
</dd>
</dl>
<dl>
<dt><strong>Default Power Limit</strong></dt>
<dd>
<p>The default power management algorithm's power ceiling, in watts.
Module Power Limit will be set back to Default Power Limit after driver
unload.</p>
</dd>
</dl>
<dl>
<dt><strong>Min Power Limit</strong></dt>
<dd>
<p>The minimum value in watts that module power limit can be set to.</p>
</dd>
</dl>
<dl>
<dt><strong>Max Power Limit</strong></dt>
<dd>
<p>The maximum value in watts that module power limit can be set to.</p>
</dd>
</dl>
<h2>GPU Memory Power Readings</h2>
<p>Information about GPU memory power consumption.</p>
<dl>
<dt><strong>Average Power Draw</strong></dt>
<dd>
<p>The average power draw for the GPU memory subsystem over the last
second, in watts.</p>
</dd>
</dl>
<dl>
<dt><strong>Instantaneous Power Draw</strong></dt>
<dd>
<p>The last measured power draw for the GPU memory subsystem, in
watts.</p>
</dd>
</dl>
<h2>Power Smoothing</h2>
<p>Power Smoothing related definitions and currently set values. This
feature allows users to tune power parameters to minimize power
fluctuations in large datacenter environments.</p>
<dl>
<dt><strong>Enabled</strong></dt>
<dd>
<p>Value is "Yes" if the feature is enabled and "No" if the feature is
not enabled.</p>
</dd>
</dl>
<dl>
<dt><strong>Delayed Power Smoothing Supported</strong></dt>
<dd>
<p>Value is "Yes" if the Delayed Power Smoothing feature is supported
and "No" if the feature is not supported.</p>
</dd>
</dl>
<dl>
<dt><strong>Privilege Level</strong></dt>
<dd>
<p>The current privilege for the user. Value is 0, 1 or 2. Note that the
higher the privilege level, the more information the user will have
access to.</p>
</dd>
</dl>
<dl>
<dt><strong>Immediate Ramp Down</strong></dt>
<dd>
<p>Values are "Enabled" or "Disabled". Indicates if ramp down hysteresis
value will be honored (when enabled) or ignored (when disabled).</p>
</dd>
</dl>
<dl>
<dt><strong>Current TMP</strong></dt>
<dd>
<p>The last read value of the Total Module Power, in watts.</p>
</dd>
</dl>
<dl>
<dt><strong>Current TMP FLoor</strong></dt>
<dd>
<p>The last read value of the Total Module Power floor, in watts.</p>
</dd>
</dl>
<dl>
<dt><strong>Max % TMP Floor</strong></dt>
<dd>
<p>The highest percentage value for which the Percent TMP Floor can be
set.</p>
</dd>
</dl>
<dl>
<dt><strong>Min % TMP Floor</strong></dt>
<dd>
<p>The lowest percentage value for which the Percent TMP Floor can be
set.</p>
</dd>
</dl>
<dl>
<dt><strong>HW Lifetime % Remaining</strong></dt>
<dd>
<p>As this feature is used, the circuitry which drives the feature wears
down. This value gives the percentage of the remaining lifetime of this
hardware.</p>
</dd>
</dl>
<dl>
<dt><strong>Current Primary Power Floor</strong></dt>
<dd>
<p>The current value of the primary power floor, in watts. This value is
calculated by doing TMP Ceiling * (% TMP FLoor value).</p>
</dd>
</dl>
<dl>
<dt><strong>Current Secondary Power Floor</strong></dt>
<dd>
<p>The current value of the secondary power floor, in watts. This is the
power floor that is applied during active workload periods on the GPU
when primary floor activation window multiplier is set to a non-zero
value.</p>
</dd>
</dl>
<dl>
<dt><strong>Min Primary Floor Activation Offset</strong></dt>
<dd>
<p>This is the minimum primary floor activation offset accepted by the
driver specified in watts. This is a static field.</p>
</dd>
</dl>
<dl>
<dt><strong>Min Primary Floor Activation Point</strong></dt>
<dd>
<p>This is the minimum absolute raw value specified in watts that the
driver will use for switching between primary and secondary floor. This
point is calculated as 'secondary power floor + primary floor activation
offset', and then computed value is floored to 'min primary floor
activation point' by the driver at run time. This value is used to avoid
setting of switch point too low accidentally.</p>
</dd>
</dl>
<dl>
<dt><strong>Window Multiplier</strong></dt>
<dd>
<p>This is the multiplier unit specified in ms for other multipliers in
the profile (primary floor activation window multiplier and primary
floor target window multiplier). This is a static field.</p>
</dd>
</dl>
<dl>
<dt><strong>Number of Preset Profiles</strong></dt>
<dd>
<p>This value is the total number of Preset Profiles supported.</p>
</dd>
</dl>
<h2>Current Profile</h2>
<p>Values for the currently acvive power smoothing preset profile.</p>
<dl>
<dt>**% TMP Floor**</dt>
<dd>
<p>The percentage of the TMP Ceiling, which is used to set the TMP
floor, for the currently active preset profile. For example, if max TMP
is 1000 W, and the % TMP floor is 50%, then the min TMP value will be
500 W. This value is in the range [Min % TMP Floor, Max % TMP
Floor].</p>
</dd>
</dl>
<dl>
<dt><strong>Ramp Up Rate</strong></dt>
<dd>
<p>The ramp up rate, measured in mW/s, for the currently active preset
profile.</p>
</dd>
</dl>
<dl>
<dt><strong>Ramp Down Rate</strong></dt>
<dd>
<p>The ramp down rate, measured in mW/s, for the currently active preset
profile.</p>
</dd>
</dl>
<dl>
<dt><strong>Ramp Down Hysteresis</strong></dt>
<dd>
<p>The ramp down hysteresis value, in ms, for the currently active
preset profile.</p>
</dd>
</dl>
<dl>
<dt><strong>Secondary Power Floor</strong></dt>
<dd>
<p>The secondary power floor, measured in watts, for the currently
active preset profile. This is the power floor that will be applied
during active workload periods on the GPU when primary floor activation
window multiplier is set to a non-zero value.</p>
</dd>
</dl>
<dl>
<dt><strong>Primary Floor Activation Window Multiplier</strong></dt>
<dd>
<p>The time multiplier for the activation moving average window size for
the currently active preset profile. This is the 'X' ms time multiplier
for the activation moving average window size. The activation moving
average is compared against the (secondary floor + primary floor
activation offset value) to determine if the controller should switch
from the secondary floor to the primary floor. Setting this to 0 will
disable switching to the secondary floor.</p>
</dd>
</dl>
<dl>
<dt><strong>Primary Floor Target Window Multiplier</strong></dt>
<dd>
<p>The time multiplier for the target moving average window size for the
currently active preset profile. This is the 'X' ms time multiplier for
the target moving average window size. When set to non-zero value, the
target moving average power determines the primary floor. When set to 0,
driver will use the Floor percentage instead to derive the primary
floor.</p>
</dd>
</dl>
<dl>
<dt><strong>Primary Floor Activation Offset</strong></dt>
<dd>
<p>The primary Floor Activation Offset, measured in watts, for the
currently active preset profile. If the target moving average falls
below the secondary floor plus this offset, the primary floor will be
activated.</p>
</dd>
</dl>
<dl>
<dt><strong>Active Preset Profile Number</strong></dt>
<dd>
<p>The number of the active preset profile.</p>
</dd>
</dl>
<h2>Admin Overrides</h2>
<p>Admin overrides allow users with sufficient permissions to preempt
the values of the currently active preset profile. If an admin override
is set for one of the fields, then this value will be used instead of
any other configured value.</p>
<dl>
<dt>**% TMP Floor**</dt>
<dd>
<p>The admin override value for % TMP Floor. This value is in the range
[Min % TMP Floor, Max % TMP Floor].</p>
</dd>
</dl>
<dl>
<dt><strong>Ramp Up Rate</strong></dt>
<dd>
<p>The admin override value for ramp up rate, measured in mW/s.</p>
</dd>
</dl>
<dl>
<dt><strong>Ramp Down Rate</strong></dt>
<dd>
<p>The admin override value for ramp down rate, measured in mW/s.</p>
</dd>
</dl>
<dl>
<dt><strong>Ramp Down Hysteresis</strong></dt>
<dd>
<p>The admin override value for ramp down hysteresis value, in ms.</p>
</dd>
</dl>
<dl>
<dt><strong>Secondary Power Floor</strong></dt>
<dd>
<p>The admin override value for secondary power floor, measured in
watts. This is the power floor that will be applied during active
workload periods on the GPU when primary floor activation window
multiplier is set to a non-zero value.</p>
</dd>
</dl>
<dl>
<dt><strong>Primary Floor Activation Window Multiplier</strong></dt>
<dd>
<p>The admin override value for primary time multiplier for the
activation moving average window size. This is the 'X' ms time
multiplier for the activation moving average window size. The activation
moving average is compared against the (secondary floor + primary floor
activation offset value) to determine if the controller should switch
from the secondary floor to the primary floor. Setting this to 0 will
disable switching to the secondary floor.</p>
</dd>
</dl>
<dl>
<dt><strong>Primary Floor Target Window Multiplier</strong></dt>
<dd>
<p>The admin override value for primary time multiplier for the target
moving average window size. This is the 'X' ms time multiplier for the
target moving average window size. When set to non-zero value, the
target moving average power determines the primary floor. When set to 0,
driver will use the Floor percentage instead to derive the primary
floor.</p>
</dd>
</dl>
<dl>
<dt><strong>Primary Floor Activation Offset</strong></dt>
<dd>
<p>The admin override value for primary Floor Activation Offset,
measured in watts. If the target moving average falls below the
secondary floor plus this offset, the primary floor will be
activated.</p>
</dd>
</dl>
<h2>Workload Power Profiles</h2>
<p>Pre-tuned GPU profiles help to provide immediate, optimized
configurations for Datacenter use cases. This sections includes
information about the currently requested on enfornced power
profiles.</p>
<dl>
<dt><strong>Requested Profiles</strong></dt>
<dd>
<p>The list of user requested profiles.</p>
</dd>
</dl>
<dl>
<dt><strong>Enforced Profiles</strong></dt>
<dd>
<p>Since many of the profiles have conflicting goals, some
configurations of requested profiles are incompatible. This is the list
of the requested profiles which are currently enforced.</p>
</dd>
</dl>
<h2>EDPp Multiplier</h2>
<p>The EDPp multiplier expressed as a percentage. This feature is meant
for system administrators and cannot be configured via NVML or
nvidia-smi.</p>
<h2>Clocks</h2>
<p>Current frequency at which parts of the GPU are running. All readings
are in MHz. Note that it is possible for clocks to report a lower
freqency than the lowest frequency that can be set by SW due to HW
optimizations in certain scenarios.</p>
<dl>
<dt><strong>Graphics</strong></dt>
<dd>
<p>Current frequency of graphics (shader) clock.</p>
</dd>
</dl>
<dl>
<dt><strong>SM</strong></dt>
<dd>
<p>Current frequency of SM (Streaming Multiprocessor) clock.</p>
</dd>
</dl>
<dl>
<dt><strong>Memory</strong></dt>
<dd>
<p>Current frequency of memory clock.</p>
</dd>
</dl>
<dl>
<dt><strong>Video</strong></dt>
<dd>
<p>Current frequency of video (encoder + decoder) clocks.</p>
</dd>
</dl>
<h2>Applications Clocks</h2>
<p>Applications Clocks will be removed in a future CUDA release. Please
use -lmc/-lgc for locking memory/graphics clocks and -rmc/-rgc to reset
memory/graphcis clocks. User specified frequency at which applications
will be running at. Can be changed with [-ac \| --applications-clocks]
switches.</p>
<dl>
<dt><strong>Graphics</strong></dt>
<dd>
<p>User specified frequency of graphics (shader) clock.</p>
</dd>
</dl>
<dl>
<dt><strong>Memory</strong></dt>
<dd>
<p>User specified frequency of memory clock.</p>
</dd>
</dl>
<h2>Default Applications Clocks</h2>
<p>Default frequency at which applications will be running at.
Application clocks can be changed with [-ac \| --applications-clocks]
switches. Application clocks can be set to default using [-rac \|
--reset-applications-clocks] switches.</p>
<dl>
<dt><strong>Graphics</strong></dt>
<dd>
<p>Default frequency of applications graphics (shader) clock.</p>
</dd>
</dl>
<dl>
<dt><strong>Memory</strong></dt>
<dd>
<p>Default frequency of applications memory clock.</p>
</dd>
</dl>
<h2>Deferred Clocks</h2>
<dl>
<dt>Deferred clocks are clocks that will be applied after the next
driver load. <strong>Memory</strong></dt>
<dd>
<p>The Memory Clock value in MHz that takes effect the next time the GPU
is initialized. This can be guaranteed by unloading and reloading the
kernel module.</p>
</dd>
</dl>
<h2>Max Clocks</h2>
<p>Maximum frequency at which parts of the GPU are design to run. All
readings are in MHz. Current P0 clocks (reported in Clocks section) can
differ from max clocks by few MHz.</p>
<dl>
<dt><strong>Graphics</strong></dt>
<dd>
<p>Maximum frequency of graphics (shader) clock.</p>
</dd>
</dl>
<dl>
<dt><strong>SM</strong></dt>
<dd>
<p>Maximum frequency of SM (Streaming Multiprocessor) clock.</p>
</dd>
</dl>
<dl>
<dt><strong>Memory</strong></dt>
<dd>
<p>Maximum frequency of memory clock.</p>
</dd>
</dl>
<dl>
<dt><strong>Video</strong></dt>
<dd>
<p>Maximum frequency of video (encoder + decoder) clock.</p>
</dd>
</dl>
<h2>Max Customer Boost Clocks</h2>
<p>Maximum customer boost frequency at which parts of the GPU are
designed to run. All readings are in MHz.</p>
<dl>
<dt><strong>Graphics</strong></dt>
<dd>
<p>Maximum customer boost frequency of graphics (shader) clock.</p>
</dd>
</dl>
<h2>Clock Policy</h2>
<p>User-specified settings for automated clocking changes such as auto
boost.</p>
<dl>
<dt><strong>Auto Boost</strong></dt>
<dd>
<p>Indicates whether auto boost mode is currently enabled for this GPU
(On) or disabled for this GPU (Off). Shows (N/A) if boost is not
supported. Auto boost allows dynamic GPU clocking based on power,
thermal and utilization. When auto boost is disabled the GPU will
attempt to maintain clocks at precisely the Current Application Clocks
settings (whenever a CUDA context is active). With auto boost enabled
the GPU will still attempt to maintain this floor, but will
opportunistically boost to higher clocks when power, thermal and
utilization headroom allow. This setting persists for the life of the
CUDA context for which it was requested. Apps can request a particular
mode either via an NVML call (see NVML SDK) or by setting the CUDA
environment variable CUDA_AUTO_BOOST. This feature is deprecated and
will be removed in a future CUDA release.</p>
</dd>
</dl>
<dl>
<dt><strong>Auto Boost Default</strong></dt>
<dd>
<p>Indicates the default setting for auto boost mode, either enabled
(On) or disabled (Off). Shows (N/A) if boost is not supported. Apps will
run in the default mode if they have not explicitly requested a
particular mode. Note: Auto Boost settings can only be modified if
"Persistence Mode" is enabled, which is NOT by default. This feature is
deprecated and will be removed in a future CUDA release.</p>
</dd>
</dl>
<h2>Fabric</h2>
<p>GPU Fabric information</p>
<p><strong>State</strong></p>
<p>Indicates the state of the GPU's handshake with the
nvidia-fabricmanager (a.k.a. GPU fabric probe)<br />
Possible values: Completed, In Progress, Not Started, Not supported</p>
<p><strong>Status</strong></p>
<p>Status of the GPU fabric probe response from the
nvidia-fabricmanager.<br />
Possible values: NVML_SUCCESS or one of the failure codes.</p>
<p><strong>Clique ID</strong></p>
<p>A clique is a set of GPUs that can communicate to each other over
NVLink.<br />
The GPUs belonging to the same clique share the same clique ID.<br />
Clique ID will only be valid for NVLink multi-node systems.</p>
<p><strong>Cluster UUID</strong></p>
<p>UUID of an NVLink multi-node cluster to which this GPU belongs.<br />
Cluster UUID will be zero for NVLink single-node systems.</p>
<p><strong>Health</strong></p>
<p>Summary - Summary of Fabric Health &lt;Healthy, Unhealthy, Limited
Capacity&gt;<br />
Bandwidth - is the GPU NVLink bandwidth degraded or not
&lt;True/False&gt;<br />
Route Recovery in progress - is NVLink route recovery in progress
&lt;True/False&gt;<br />
Route Unhealthy - is NVLink route recovery failed or aborted
&lt;True/False&gt;<br />
Access Timeout Recovery - is NVLink access timeout recovery in progress
&lt;True/False&gt;<br />
Incorrect Configuration - Incorrect Configuration status &lt;Incorrect
SystemGuid, Incorrect Chassis Serial Number, No Partition, Insufficient
Nvlink Resources, Incompatible GPU Firmware, Invalid Location,
None&gt;</p>
<h2>Processes</h2>
<p>List of processes having Compute or Graphics Context on the device.
Compute processes are reported on all the fully supported products.
Reporting for Graphics processes is limited to the supported products
starting with Kepler architecture.</p>
<dl>
<dt>Each Entry is of format "&lt;GPU Index&gt; &lt;GI Index&gt; &lt;CI
Index&gt; &lt;PID&gt; &lt;Type&gt; &lt;Process Name&gt; &lt;GPU Memory
Usage&gt;"</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>GPU Index</strong></dt>
<dd>
<p>Represents NVML Index of the device.</p>
</dd>
</dl>
<dl>
<dt><strong>GPU Instance Index</strong></dt>
<dd>
<p>Represents GPU Instance Index of the MIG device (if enabled).</p>
</dd>
</dl>
<dl>
<dt><strong>Compute Instance Index</strong></dt>
<dd>
<p>Represents Compute Instance Index of the MIG device (if enabled).</p>
</dd>
</dl>
<dl>
<dt><strong>PID</strong></dt>
<dd>
<p>Represents Process ID corresponding to the active Compute or Graphics
context.</p>
</dd>
</dl>
<dl>
<dt><strong>Type</strong></dt>
<dd>
<p>Displayed as "C" for Compute Process, "G" for Graphics Process, "M"
for MPS ("Multi-Process Service") Compute Process, and "C+G" or "M+C"
for the process having both Compute and Graphics or MPS Compute and
Compute contexts.</p>
</dd>
</dl>
<dl>
<dt><strong>Process Name</strong></dt>
<dd>
<p>Represents process name for the Compute, MPS Compute, or Graphics
process.</p>
</dd>
</dl>
<dl>
<dt><strong>GPU Memory Usage</strong></dt>
<dd>
<p>Amount of memory used by the GPU context, which represents FB memory
usage for discrete GPUs or system memory usage for integrated GPUs. Not
available on Windows when running in WDDM mode because Windows KMD
manages all the memory not NVIDIA driver.</p>
</dd>
</dl>
<h2>Device Monitoring</h2>
<p>The "nvidia-smi dmon" command-line is used to monitor one or more
GPUs (up to 16 devices) plugged into the system. This tool allows the
user to see one line of monitoring data per monitoring cycle. The output
is in concise format and easy to interpret in interactive mode. The
output data per line is limited by the terminal size. It is supported on
Tesla, GRID, Quadro and limited GeForce products for Kepler or newer
GPUs under bare metal 64 bits Linux. By default, the monitoring data
includes Power Usage, Temperature, SM clocks, Memory clocks and
Utilization values for SM, Memory, Encoder, Decoder, JPEG and OFA. It
can also be configured to report other metrics such as frame buffer
memory usage, bar1 memory usage, power/thermal violations and aggregate
single/double bit ecc errors. If any of the metric is not supported on
the device or any other error in fetching the metric is reported as "-"
in the output data. The user can also configure monitoring frequency and
the number of monitoring iterations for each run. There is also an
option to include date and time at each line. All the supported options
are exclusive and can be used together in any order. Note: On
MIG-enabled GPUs, querying the utilization of encoder, decoder, jpeg,
ofa, gpu, and memory is not currently supported.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Default with no arguments</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi dmon</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Monitors default metrics for up to 16 supported devices under
natural enumeration (starting with GPU index 0) at a frequency of 1 sec.
Runs until terminated with ^C.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Select one or more devices</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi dmon -i &lt;device1,device2, .. ,
deviceN&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Reports default metrics for the devices selected by comma separated
device list. The tool picks up to 16 supported devices from the list
under natural enumeration (starting with GPU index 0).</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) Select metrics to be displayed</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi dmon -s &lt;metric_group&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>&lt;metric_group&gt; can be one or more from the following:</dt>
<dd>

</dd>
</dl>
<dl>
<dt>p - Power Usage (in Watts) and GPU/Memory Temperature (in C) if
supported</dt>
<dd>

</dd>
</dl>
<dl>
<dt>u - Utilization (SM, Memory, Encoder, Decoder, JPEG and OFA
Utilization in %)</dt>
<dd>

</dd>
</dl>
<dl>
<dt>c - Proc and Mem Clocks (in MHz)</dt>
<dd>

</dd>
</dl>
<dl>
<dt>v - Power Violations (in %) and Thermal Violations (as a boolean
flag)</dt>
<dd>

</dd>
</dl>
<dl>
<dt>m - Frame Buffer, Bar1 and Confidential Compute protected memory
usage (in MB)</dt>
<dd>

</dd>
</dl>
<dl>
<dt>e - ECC (Number of aggregated single bit, double bit ecc errors) and
PCIe Replay errors</dt>
<dd>

</dd>
</dl>
<dl>
<dt>t - PCIe Rx and Tx Throughput in MB/s (Maxwell and above)</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Configure monitoring iterations</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi dmon -c &lt;number of samples&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays data for specified number of samples and exit.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>5) Configure monitoring frequency</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi dmon -d &lt;time in secs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Collects and displays data at every specified monitoring interval
until terminated with ^C.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>6) Display date</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi dmon -o D</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Prepends monitoring data with date in YYYYMMDD format.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>7) Display time</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi dmon -o T</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Prepends monitoring data with time in HH:MM:SS format.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>8) Select GPM metrics to be displayed</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi dmon --gpm-metrics
&lt;gpmMetric1,gpmMetric2,...,gpmMetricN&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>&lt;gpmMetricX&gt; Refer to the documentation for nvmlGpmMetricId_t
in the NVML header file</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>9) Select which level of GPM metrics to be
displayed</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi dmon --gpm-options &lt;gpmMode&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>&lt;gpmMode&gt; can be one of the following:</dt>
<dd>

</dd>
</dl>
<dl>
<dt>d - Display Device Level GPM metrics</dt>
<dd>

</dd>
</dl>
<dl>
<dt>m - Display MIG Level GPM metrics</dt>
<dd>

</dd>
</dl>
<dl>
<dt>dm - Display Device and MIG Level GPM metrics</dt>
<dd>

</dd>
</dl>
<dl>
<dt>md - Display Device and MIG Level GPM metrics, same as 'dm'</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>10) Modify output format</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi dmon --format &lt;formatSpecifier&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>&lt;formatSpecifier&gt; can be any comma separated combination of
the following:</dt>
<dd>

</dd>
</dl>
<dl>
<dt>csv - Format dmon output as CSV</dt>
<dd>

</dd>
</dl>
<dl>
<dt>nounit - Remove unit line from dmon output</dt>
<dd>

</dd>
</dl>
<dl>
<dt>noheader - Remove header line from dmon output</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>11) Help Information</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi dmon -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help information for using the command line.</dt>
<dd>

</dd>
</dl>
<h2>Daemon (EXPERIMENTAL)</h2>
<p>The "nvidia-smi daemon" starts a background process to monitor one or
more GPUs plugged in to the system. It monitors the requested GPUs every
monitoring cycle and logs the file in compressed format at the user
provided path or the default location at /var/log/nvstats/. The log file
is created with system's date appended to it and of the format
nvstats-YYYYMMDD. The flush operation to the log file is done every
alternate monitoring cycle. Daemon also logs it's own PID at
/var/run/nvsmi.pid. By default, the monitoring data to persist includes
Power Usage, Temperature, SM clocks, Memory clocks and Utilization
values for SM, Memory, Encoder, Decoder, JPEG and OFA. The daemon tools
can also be configured to record other metrics such as frame buffer
memory usage, bar1 memory usage, power/thermal violations and aggregate
single/double bit ecc errors.The default monitoring cycle is set to 10
secs and can be configured via command-line. It is supported on Tesla,
GRID, Quadro and GeForce products for Kepler or newer GPUs under bare
metal 64 bits Linux. The daemon requires root privileges to run, and
only supports running a single instance on the system. All of the
supported options are exclusive and can be used together in any order.
Note: On MIG-enabled GPUs, querying the utilization of encoder, decoder,
jpeg, ofa, gpu, and memory is not currently supported.
<strong>Usage:</strong></p>
<dl>
<dt><strong>1) Default with no arguments</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi daemon</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Runs in the background to monitor default metrics for up to 16
supported devices under natural enumeration (starting with GPU index 0)
at a frequency of 10 sec. The date stamped log file is created at
/var/log/nvstats/.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Select one or more devices</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi daemon -i &lt;device1,device2, .. ,
deviceN&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Runs in the background to monitor default metrics for the devices
selected by comma separated device list. The tool picks up to 16
supported devices from the list under natural enumeration (starting with
GPU index 0).</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) Select metrics to be monitored</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi daemon -s &lt;metric_group&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>&lt;metric_group&gt; can be one or more from the following:</dt>
<dd>

</dd>
</dl>
<dl>
<dt>p - Power Usage (in Watts) and GPU/Memory Temperature (in C) if
supported</dt>
<dd>

</dd>
</dl>
<dl>
<dt>u - Utilization (SM, Memory, Encoder, Decoder, JPEG and OFA
Utilization in %)</dt>
<dd>

</dd>
</dl>
<dl>
<dt>c - Proc and Mem Clocks (in MHz)</dt>
<dd>

</dd>
</dl>
<dl>
<dt>v - Power Violations (in %) and Thermal Violations (as a boolean
flag)</dt>
<dd>

</dd>
</dl>
<dl>
<dt>m - Frame Buffer, Bar1 and Confidential Compute protected memory
usage (in MB)</dt>
<dd>

</dd>
</dl>
<dl>
<dt>e - ECC (Number of aggregated single bit, double bit ecc errors) and
PCIe Replay errors</dt>
<dd>

</dd>
</dl>
<dl>
<dt>t - PCIe Rx and Tx Throughput in MB/s (Maxwell and above)</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Configure monitoring frequency</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi daemon -d &lt;time in secs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Collects data at every specified monitoring interval until
terminated.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>5) Configure log directory</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi daemon -p &lt;path of directory&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>The log files are created at the specified directory.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>6) Configure log file name</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi daemon -j &lt;string to append log file
name&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>The command-line is used to append the log file name with the user
provided string.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>7) Terminate the daemon</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi daemon -t</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>This command-line uses the stored PID (at /var/run/nvsmi.pid) to
terminate the daemon. It makes the best effort to stop the daemon and
offers no guarantees for it's termination. In case the daemon is not
terminated, then the user can manually terminate by sending kill signal
to the daemon. Performing a GPU reset operation (via nvidia-smi)
requires all GPU processes to be exited, including the daemon. Users who
have the daemon open will see an error to the effect that the GPU is
busy.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>8) Help Information</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi daemon -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help information for using the command line.</dt>
<dd>

</dd>
</dl>
<h2>Replay Mode (EXPERIMENTAL)</h2>
<p>The "nvidia-smi replay" command-line is used to extract/replay all or
parts of log file generated by the daemon. By default, the tool tries to
pull the metrics such as Power Usage, Temperature, SM clocks, Memory
clocks and Utilization values for SM, Memory, Encoder, Decoder, JPEG and
OFA. The replay tool can also fetch other metrics such as frame buffer
memory usage, bar1 memory usage, power/thermal violations and aggregate
single/double bit ecc errors. There is an option to select a set of
metrics to replay, If any of the requested metric is not maintained or
logged as not-supported then it's shown as "-" in the output. The format
of data produced by this mode is such that the user is running the
device monitoring utility interactively. The command line requires
mandatory option "-f" to specify complete path of the log filename, all
the other supported options are exclusive and can be used together in
any order. Note: On MIG-enabled GPUs, querying the utilization of
encoder, decoder, jpeg, ofa, gpu, and memory is not currently supported.
<strong>Usage:</strong></p>
<dl>
<dt><strong>1) Specify log file to be replayed</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi replay -f &lt;log file name&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Fetches monitoring data from the compressed log file and allows the
user to see one line of monitoring data (default metrics with
time-stamp) for each monitoring iteration stored in the log file. A new
line of monitoring data is replayed every other second irrespective of
the actual monitoring frequency maintained at the time of collection. It
is displayed till the end of file or until terminated by ^C.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Filter metrics to be replayed</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi replay -f &lt;path to log file&gt; -s
&lt;metric_group&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>&lt;metric_group&gt; can be one or more from the following:</dt>
<dd>

</dd>
</dl>
<dl>
<dt>p - Power Usage (in Watts) and GPU/Memory Temperature (in C) if
supported</dt>
<dd>

</dd>
</dl>
<dl>
<dt>u - Utilization (SM, Memory, Encoder, Decoder, JPEG and OFA
Utilization in %)</dt>
<dd>

</dd>
</dl>
<dl>
<dt>c - Proc and Mem Clocks (in MHz)</dt>
<dd>

</dd>
</dl>
<dl>
<dt>v - Power Violations (in %) and Thermal Violations (as a boolean
flag)</dt>
<dd>

</dd>
</dl>
<dl>
<dt>m - Frame Buffer, Bar1 and Confidential Compute protected memory
usage (in MB)</dt>
<dd>

</dd>
</dl>
<dl>
<dt>e - ECC (Number of aggregated single bit, double bit ecc errors) and
PCIe Replay errors</dt>
<dd>

</dd>
</dl>
<dl>
<dt>t - PCIe Rx and Tx Throughput in MB/s (Maxwell and above)</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) Limit replay to one or more devices</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi replay -f &lt;log file&gt; -i &lt;device1,device2, ..
, deviceN&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Limits reporting of the metrics to the set of devices selected by
comma separated device list. The tool skips any of the devices not
maintained in the log file.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Restrict the time frame between which data is
reported</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi replay -f &lt;log file&gt; -b &lt;start time in
HH:MM:SS format&gt; -e &lt;end time in HH:MM:SS format&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>This option allows the data to be limited between the specified time
range. Specifying time as 0 with -b or -e option implies start or end
file respectively.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>5) Redirect replay information to a log file</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi replay -f &lt;log file&gt; -r &lt;output file
name&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>This option takes log file as an input and extracts the information
related to default metrics in the specified output file.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>6) Help Information</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi replay -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help information for using the command line.</dt>
<dd>

</dd>
</dl>
<h2>Process Monitoring</h2>
<p>The "nvidia-smi pmon" command-line is used to monitor compute and
graphics processes running on one or more GPUs (up to 16 devices)
plugged into the system. This tool allows the user to see the statistics
for all the running processes on each device at every monitoring cycle.
The output is in concise format and easy to interpret in interactive
mode. The output data per line is limited by the terminal size. It is
supported on Tesla, GRID, Quadro and limited GeForce products for Kepler
or newer GPUs under bare metal 64 bits Linux. By default, the monitoring
data for each process includes the pid, command name and average
utilization values for SM, Memory, Encoder and Decoder since the last
monitoring cycle. It can also be configured to report frame buffer
memory usage for each process. If there is no process running for the
device, then all the metrics are reported as "-" for the device. If any
of the metric is not supported on the device or any other error in
fetching the metric is also reported as "-" in the output data. The user
can also configure monitoring frequency and the number of monitoring
iterations for each run. There is also an option to include date and
time at each line. All the supported options are exclusive and can be
used together in any order. Note: On MIG-enabled GPUs, querying the
utilization of encoder, decoder, jpeg, ofa, gpu, and memory is not
currently supported.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Default with no arguments</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi pmon</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Monitors all the processes running on each device for up to 16
supported devices under natural enumeration (starting with GPU index 0)
at a frequency of 1 sec. Runs until terminated with ^C.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Select one or more devices</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi pmon -i &lt;device1,device2, .. ,
deviceN&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Reports statistics for all the processes running on the devices
selected by comma separated device list. The tool picks up to 16
supported devices from the list under natural enumeration (starting with
GPU index 0).</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) Select metrics to be displayed</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi pmon -s &lt;metric_group&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>&lt;metric_group&gt; can be one or more from the following:</dt>
<dd>

</dd>
</dl>
<dl>
<dt>u - Utilization (SM, Memory, Encoder, Decoder, JPEG, and OFA
Utilization for the process in %). Reports average utilization since
last monitoring cycle.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>m - Frame Buffer and Confidential Compute protected memory usage (in
MB). Reports instantaneous value for memory usage.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Configure monitoring iterations</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi pmon -c &lt;number of samples&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays data for specified number of samples and exit.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>5) Configure monitoring frequency</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi pmon -d &lt;time in secs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Collects and displays data at every specified monitoring interval
until terminated with ^C. The monitoring frequency must be between 1 to
10 secs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>6) Display date</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi pmon -o D</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Prepends monitoring data with date in YYYYMMDD format.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>7) Display time</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi pmon -o T</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Prepends monitoring data with time in HH:MM:SS format.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>8) Help Information</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi pmon -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help information for using the command line.</dt>
<dd>

</dd>
</dl>
<h2>Topology</h2>
<p>List topology information about the system's GPUs, how they connect
to each other, their CPU and memory affinities as well as qualified NICs
capable of RDMA.</p>
<p>Note: On some systems, a NIC is used as a PCI bridge for the NVLINK
switches and is not useful from a networking or RDMA point of view. The
nvidia-smi topo command will filter the NIC's ports/PCIe sub-functions
out of the topology matrix by examining the NIC's sysfs entries. On some
kernel versions, nvidia-smi requires root privileges to read these sysfs
entries.</p>
<dl>
<dt><strong>Usage:</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>Topology connections and affinities matrix between the GPUs
and NICs in the system</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi topo -m</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays a matrix of connections between all GPUs and NICs(including
their data-direct devices if applicable) in the system along with
CPU/memory affinities for the GPUs with the following legend:</dt>
<dd>

</dd>
</dl>
<p>Legend:<br />
X = Self<br />
SYS = Connection traversing PCIe as well as the SMP interconnect between
NUMA nodes (e.g., QPI/UPI)<br />
NODE = Connection traversing PCIe as well as the interconnect between
PCIe Host Bridges within a NUMA node<br />
PHB = Connection traversing PCIe as well as a PCIe Host Bridge
(typically the CPU)<br />
PXB = Connection traversing multiple PCIe switches (without traversing
the PCIe Host Bridge)<br />
PIX = Connection traversing a single PCIe switch NV# = Connection
traversing a bonded set of # NVLinks<br />
</p>
<dl>
<dt>Note: This command may also display bonded NICs which may not be
RDMA capable.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi topo -mp</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays a matrix of PCI-only connections between all GPUs and NICs
in the system along with CPU/memory affinities for the GPUs with the
same legend as the 'nvidia-smi topo -m' command. This command excludes
NVLINK connections and shows PCI connections between GPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi topo -c &lt;CPU number&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Shows all the GPUs with an affinity to the specified CPU
number.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi topo -n &lt;traversal_path&gt; -i
&lt;deviceID&gt;</em></dt>
<dd>

</dd>
</dl>
<p>Shows all the GPUs connected with the given GPU using the specified
traversal path. The traversal path values are:<br />
0 = A single PCIe switch on a dual GPU board<br />
1 = A single PCIe switch<br />
2 = Multiple PCIe switches<br />
3 = A PCIe host bridge<br />
4 = An on-CPU interconnect link between PCIe host bridges<br />
5 = An SMP interconnect link between NUMA nodes<br />
</p>
<blockquote>

</blockquote>
<dl>
<dt><em>nvidia-smi topo -p -i
&lt;deviceID1&gt;,&lt;deviceID2&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Shows the most direct PCIe path traversal for a given pair of
GPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi topo -p2p &lt;capability&gt;</em></dt>
<dd>

</dd>
</dl>
<p>Shows the P2P status between all GPUs, given a capability. Capability
values are:<br />
r - p2p read capability<br />
w - p2p write capability<br />
n - p2p nvlink capability<br />
a - p2p atomics capability<br />
p - p2p pcie capability<br />
</p>
<blockquote>

</blockquote>
<dl>
<dt><em>nvidia-smi topo -C -i &lt;deviceID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Shows the NUMA ID of the nearest CPU for a GPU represented by the
device ID.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi topo -M -i &lt;deviceID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Shows the NUMA ID of the nearest memory for a GPU represented by the
device ID.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi topo -gnid -i &lt;deviceID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Shows the NUMA ID of the GPU represented by the device ID, if
applicable. Displays N/A otherwise.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi topo -nvme</em></dt>
<dd>

</dd>
</dl>
<p>Displays a matrix of PCI connections between all GPUs and NVME
devices in the system with the following legend:</p>
<p>Legend:<br />
X = Self<br />
SYS = Connection traversing PCIe as well as the SMP interconnect between
NUMA nodes (e.g., QPI/UPI)<br />
NODE = Connection traversing PCIe as well as the interconnect between
PCIe Host Bridges within a NUMA node<br />
PHB = Connection traversing PCIe as well as a PCIe Host Bridge
(typically the CPU)<br />
PXB = Connection traversing multiple PCIe bridges (without traversing
the PCIe Host Bridge)<br />
PIX = Connection traversing at most a single PCIe bridge<br />
</p>
<blockquote>

</blockquote>
<h2>Nvlink</h2>
<p>The "nvidia-smi nvlink" command-line is used to manage the GPU's
Nvlinks. It provides options to set and query Nvlink information.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Display help menu</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help menu for using the command-line.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) List one or more GPUs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects one or more GPUs using the given comma-separated GPU
indexes, PCI bus IDs or UUIDs. If not used, the given command-line
option applies to all of the supported GPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) Select a specific NvLink</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -l &lt;GPU Nvlink Id&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --list &lt;GPU Nvlink Id&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects a specific Nvlink of the GPU for the given command, if
valid. If not used, the given command-line option allies to all of the
GPU's Nvlinks.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Query Nvlink Status</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -s</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --status</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get the status of the GPU's Nvlinks.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>If Active, the Bandwidth of the links will be displayed.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>If the link is present but Not Active, it will show the link as
Inactive.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>If the link is in Sleep state, it will show as Sleep.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>5) Query Nvlink capabilities</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -c</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --capabilities</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get the GPU's Nvlink capabilities.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>6) Query the Nvlink's remote node PCI bus</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -p</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -pcibusid</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get the Nvlink's remote node PCI bus ID.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>7) Query the Nvlink's remote link info</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -R</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -remotelinkinfo</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get the remote device PCI bus ID and NvLink ID for a link.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>8) Set Nvlink Counter Control is DEPRECATED</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>9) Get Nvlink Counter Control is DEPRECATED</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>10) Get Nvlink Counters is DEPRECATED, -gt/--getthroughput
should be used instead</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>11) Reset Nvlink counters is DEPRECATED</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>12) Query Nvlink Error Counters</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -e</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --errorcounters</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get the Nvlink error counters.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>For NVLink 4</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Replay Errors - count the number of replay 'events' that
occurred</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Recovery Errors - count the number of link recovery events</dt>
<dd>

</dd>
</dl>
<dl>
<dt>CRC Errors - count the number of CRC errors in received packets</dt>
<dd>

</dd>
</dl>
<dl>
<dt>For NVLink 5</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Tx packets - Total Tx packets on the link</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Tx bytes - Total Tx bytes on the link</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Rx packets - Total Rx packets on the link</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Rx bytes - Total Rx bytes on the link</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Malformed packet Errors - Number of packets Rx on a link where
packets are malformed</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Buffer overrun Errors - Number of packets that were discarded on Rx
due to buffer overrun</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Rx Errors - Total number of packets with errors Rx on a link</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Rx remote Errors - Total number of packets Rx - stomp/EBP
marker</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Rx General Errors - Total number of packets Rx with header
mismatch</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Local link integrity Errors - Total number of times that the count
of local errors exceeded a threshold</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Tx discards - Total number of tx error packets that were
discarded</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Link recovery successful events - Number of times link went from Up
to recovery, succeeded and link came back up</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Link recovery failed events - Number of times link went from Up to
recovery, failed and link was declared down</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Total link recovery events - Number of times link went from Up to
recovery, irrespective of the result</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Effective Errors - Sum of the number of errors in each Nvlink
packet</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Effective BER - BER for symbol errors</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Symbol Errors - Number of errors in rx symbols</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Symbol BER - BER for symbol errors</dt>
<dd>

</dd>
</dl>
<dl>
<dt>FEC Errors - [0-15] - count of symbol errors that are corrected</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>13) Query Nvlink CRC error counters</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -ec</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --crcerrorcounters</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get the Nvlink per-lane CRC/ECC error counters.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>CRC - NVLink 4 and before - Total Rx CRC errors on an NVLink
Lane</dt>
<dd>

</dd>
</dl>
<dl>
<dt>ECC - NVLink 4 - Total Rx ECC errors on an NVLink Lane</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Deprecated NVLink 5 onwards</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>14) Reset Nvlink Error Counters</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -re</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --reseterrorcounters</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Reset all Nvlink error counters to zero.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>NvLink 5 NOT SUPPORTED</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>15) Query Nvlink throughput counters</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -gt &lt;Data Type&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --getthroughput &lt;Data Type&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>&lt;Data Type&gt; can be one of the following:</dt>
<dd>

</dd>
</dl>
<dl>
<dt>d - Tx and Rx data payload in KiB.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>r - Tx and Rx raw payload and protocol overhead in KiB.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>16) Set Nvlink Low Power thresholds</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -sLowPwrThres &lt;Threshold&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --setLowPowerThreshold
&lt;Threshold&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Set the Nvlink Low Power Threshold, before the links go into Low
Power Mode.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>Threshold ranges and units can be found using -gLowPwrInfo.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>17) Get Nvlink Low Power Info</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -gLowPwrInfo</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --getLowPowerInfo</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Query the Nvlink's Low Power Info.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>18) Set Nvlink Bandwidth mode</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -sBwMode &lt;Bandwidth Mode&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --setBandwidthMode &lt;Bandwidth
Mode&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Set the Nvlink Bandwidth mode for all GPUs. This is DEPRECATED for
Blackwell+.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>The options are:</dt>
<dd>

</dd>
</dl>
<dl>
<dt>FULL - All links are at max Bandwidth.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>OFF - Bandwidth is not used. P2P is via PCIe bus.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>MIN - Bandwidth is at minimum speed.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>HALF - Bandwidth is at around half of FULL speed.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>3QUARTER - Bandwidth is at around 75% of FULL speed.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>19) Get Nvlink Bandwidth mode</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -gBwMode</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --getBandwidthMode</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get the Nvlink Bandwidth mode for all GPUs. THis is DEPRECATED for
Blackwell+.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>20) Query for Nvlink Bridge</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -cBridge</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --checkBridge</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Query for Nvlink Bridge presence.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>21) Set the GPU's Nvlink Width</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -sLWidth &lt;Link Width&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --setLinkWidth &lt;Link Width&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Set the GPU's Nvlink width, which will be keep those number of links
Active, and the rest to sleep.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>&lt;Link Width&gt; can be one of the following:</dt>
<dd>

</dd>
</dl>
<dl>
<dt>values - List possible Link Widths to be set.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>The numerical value from the above option.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>22) Get the GPU's Nvlink Width</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -gLWidth</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --getLinkWidth</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Query the GPU's Nvlink Width.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>23) Get the GPU's Nvlink Device Information</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink -info</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi nvlink --info</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Query the GPU's Nvlink device information.</dt>
<dd>

</dd>
</dl>
<h2>C2C</h2>
<p>The "nvidia-smi c2c" command-line is used to manage the GPU's C2C
Links. It provides options to query C2C Link information.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Display help menu</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi c2c -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help menu for using the command-line.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) List one or more GPUs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi c2c -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi c2c --id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects one or more GPUs using the given comma-separated GPU
indexes, PCI bus IDs or UUIDs. If not used, the given command-line
option applies to all of the supported GPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) Select a specific C2C Link</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi c2c -l &lt;GPU C2C Id&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi c2c --list &lt;GPU C2C Id&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects a specific C2C Link of the GPU for the given command, if
valid. If not used, the given command-line option allies to all of the
GPU's C2C Links.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Query C2C Link Status</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi c2c -s</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi c2c --status</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get the status of the GPU's C2C Links. If active, the Bandwidth of
the links will be displayed.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>5) Query C2C Link Error Counters</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi c2c -e</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi c2c -errorCounters</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display the C2C Link error counters.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>6) Query C2C Link Power Info</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi c2c -gLowPwrInfo</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi c2c -getLowPowerInfo</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display the C2C Link Power state.</dt>
<dd>

</dd>
</dl>
<h2>vGPU Management</h2>
<p>The "nvidia-smi vgpu" command reports on GRID vGPUs executing on
supported GPUs and hypervisors (refer to driver release notes for
supported platforms). Summary reporting provides basic information about
vGPUs currently executing on the system. Additional options provide
detailed reporting of vGPU properties, per-vGPU reporting of SM, Memory,
Encoder, Decoder, Jpeg, and OFA utilization, and per-GPU reporting of
supported and creatable vGPUs. Periodic reports can be automatically
generated by specifying a configurable loop frequency to any command.
Note: On MIG-enabled GPUs, querying the utilization of encoder, decoder,
jpeg, ofa, gpu, and memory is not currently supported.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Help Information</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help information for using the command line.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Default with no arguments</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Reports summary of all the vGPUs currently active on each
device.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) Display detailed info on currently active
vGPUs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -q</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Collects and displays information on currently active vGPUs on each
device, including driver version, utilization, and other
information.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Select one or more devices</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -i &lt;device1,device2, .. ,
deviceN&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Reports summary for all the vGPUs currently active on the devices
selected by comma-separated device list.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>5) Display supported vGPUs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -s</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays vGPU types supported on each device. Use the -v / --verbose
option to show detailed info on each vGPU type.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>6) Display creatable vGPUs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -c</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays vGPU types creatable on each device. This varies
dynamically, depending on the vGPUs already active on the device. Use
the -v / --verbose option to show detailed info on each vGPU type.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>7) Report utilization for currently active
vGPUs.</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -u</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Reports average utilization (SM, Memory, Encoder, Decoder, Jpeg, and
OFA) for each active vGPU since last monitoring cycle. The default cycle
time is 1 second, and the command runs until terminated with ^C. If a
device has no active vGPUs, its metrics are reported as "-".</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>8) Configure loop frequency</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu [-s -c -q -u] -l &lt;time in secs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Collects and displays data at a specified loop interval until
terminated with ^C. The loop frequency must be between 1 and 10 secs.
When no time is specified, the loop frequency defaults to 5 secs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>9) Display GPU engine usage</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -p</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display GPU engine usage of currently active processes running in
the vGPU VMs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>10) Display migration capabitlities.</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -m</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display pGPU's migration/suspend/resume capability.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>11) Display the vGPU Software scheduler state.</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -ss</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display the information about vGPU Software scheduler state.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>12) Display the vGPU Software scheduler
capabilities.</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -sc</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display the list of supported vGPU scheduler policies returned along
with the other capabilities values, if the engine is Graphics type. For
other engine types, it is BEST EFFORT policy and other capabilities will
be zero. If ARR is supported and enabled, scheduling frequency and
averaging factor are applicable else timeSlice is applicable.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>13) Display the vGPU Software scheduler logs.</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -sl</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display the vGPU Software scheduler runlist logs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi --query-vgpu-scheduler-logs=[input
parameters]</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display the vGPU Software scheduler runlist logs in CSV format.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>14) Set the vGPU Software scheduler state.</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu --set-vgpu-scheduler-state [options]</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Set the vGPU Software scheduler policy and states.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>15) Display NVIDIA Encoder session info.</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -es</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display the information about encoder sessions for currently running
vGPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>16) Display accounting statistics.</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu --query-accounted-apps=[input
parameters]</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display accounting stats for compute/graphics processes.</dt>
<dd>

</dd>
</dl>
<dl>
<dt>To find the list of properties which can be queried, run -
'nvidia-smi --help-query-accounted-apps'.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>17) Display NVIDIA Frame Buffer Capture session
info.</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -fs</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display the information about FBC sessions for currently running
vGPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>Note : Horizontal resolution, vertical resolution, average FPS
and average latency data for a FBC session may be zero if there are no
new frames captured since the session started.</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>18) Set vGPU heterogeneous mode.</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -shm</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Set vGPU heterogeneous mode of the device for timesliced vGPUs with
different framebuffer sizes.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>19) Set vGPU MIG timeslice mode.</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -smts</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Set vGPU MIG timeslice mode of the device.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>20) Display the currently creatable vGPU types on the user
provided GPU Instance</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -c -gi &lt;GPU instance IDs&gt; -i &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -c --gpu-instance-id &lt;GPU instance IDs&gt;
--id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Provide comma separated values for more than one GPU instance. The
target GPU index (MANDATORY) for the given GPU instance.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>21) Display detailed information of the currently active
vGPU instances on the user provided GPU Instance</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -q -gi &lt;GPU instance IDs&gt; -i &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -q --gpu-instance-id &lt;GPU instance IDs&gt;
--id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Provide comma separated values for more than one GPU instance. The
target GPU index (MANDATORY) for the given GPU instance.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>22) Display the vGPU scheduler state on the user provided
GPU Instance</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -ss -gi &lt;GPU instance IDs&gt; -i &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -ss --gpu-instance-id &lt;GPU instance IDs&gt;
--id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Provide comma separated values for more than one GPU instance. The
target GPU index (MANDATORY) for the given GPU instance.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>23) Get the vGPU heterogeneous mode on the user provided GPU
Instance</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -ghm -gi &lt;GPU instance IDs&gt; -i &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -ghm --gpu-instance-id &lt;GPU instance IDs&gt;
--id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Provide comma separated values for more than one GPU instance. The
target GPU index (MANDATORY) for the given GPU instance. If not used,
the given command-line option applies to all of the GPU instances.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>24) Set the vGPU heterogeneous mode on the user provided GPU
Instance</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -shm -gi &lt;GPU instance IDs&gt; -i &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -shm --gpu-instance-id &lt;GPU instance IDs&gt;
--id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Provide comma separated values for more than one GPU instance. The
target GPU index (MANDATORY) for the given GPU instance.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>25) Set the vGPU Software scheduler state on the user
provided GPU Instance.</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu set-vgpu-scheduler-state [options] -gi &lt;GPU
instance IDs&gt; -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu set-vgpu-scheduler-state [options]
--gpu-instance-id &lt;GPU instance IDs&gt; --id &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Provide comma separated values for more than one GPU instance. The
target GPU index (MANDATORY) for the given GPU instance.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>26) Display the vGPU scheduler logs on the user provided GPU
Instance</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -sl -gi &lt;GPU instance IDs&gt; -i &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -sl --gpu-instance-id &lt;GPU instance IDs&gt;
--id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Provide comma separated values for more than one GPU instance. The
target GPU index (MANDATORY) for the given GPU instance.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu --query-gpu-instance-vgpu-scheduler-logs=[input
parameters] -gi &lt;GPU instance IDs&gt; -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display the vGPU Software scheduler logs in CSV format on the user
provided GPU Instance.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>27) Display detailed information of the currently creatable
vGPU types on the user provided GPU Instance</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -c -v -gi &lt;GPU instance IDs&gt; -i &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi vgpu -c -v --gpu-instance-id &lt;GPU instance IDs&gt;
--id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Provide comma separated values for more than one GPU instance. The
target GPU index (MANDATORY) for the given GPU instance.</dt>
<dd>

</dd>
</dl>
<h2>MIG Management</h2>
<p>The privileged "nvidia-smi mig" command-line is used to manage
MIG-enabled GPUs. It provides options to create, list and destroy GPU
instances and compute instances.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Display help menu</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help menu for using the command-line.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Select one or more GPUs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects one or more GPUs using the given comma-separated GPU
indexes, PCI bus IDs or UUIDs. If not used, the given command-line
option applies to all of the supported GPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) Select one or more GPU instances</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -gi &lt;GPU instance IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --gpu-instance-id &lt;GPU instance
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects one or more GPU instances using the given comma-separated
GPU instance IDs. If not used, the given command-line option applies to
all of the GPU instances.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Select one or more compute instances</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -ci &lt;compute instance IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --compute-instance-id &lt;compute instance
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects one or more compute instances using the given
comma-separated compute instance IDs. If not used, the given
command-line option applies to all of the compute instances.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>5) List GPU instance profiles</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -lgip -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --list-gpu-instance-profiles --id &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Lists GPU instance profiles, their availability and IDs. Profiles
describe the supported types of GPU instances, including all of the GPU
resources they exclusively control.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>6) List GPU instance possible placements</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -lgipp -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --list-gpu-instance-possible-placements --id
&lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Lists GPU instance possible placements. Possible placements describe
the locations of the supported types of GPU instances within the
GPU.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>7) Create GPU instance</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -cgi &lt;GPU instance specifiers&gt; -i &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --create-gpu-instance &lt;GPU instance
specifiers&gt; --id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Creates GPU instances for the given GPU instance specifiers. A GPU
instance specifier comprises a GPU instance profile name or ID and an
optional placement specifier consisting of a colon and a placement start
index. The command fails if the GPU resources required to allocate the
requested GPU instances are not available, or if the placement index is
not valid for the given profile.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>8) Create a GPU instance along with the default compute
instance</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -cgi &lt;GPU instance profile IDs or names&gt; -i
&lt;GPU IDs&gt; -C</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --create-gpu-instance &lt;GPU instance profile
IDs or names&gt; --id &lt;GPU IDs&gt;
--default-compute-instance</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>9) List GPU instances</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -lgi -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --list-gpu-instances --id &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Lists GPU instances and their IDs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>10) Destroy GPU instance</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -dgi -gi &lt;GPU instance IDs&gt; -i &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --destroy-gpu-instances --gpu-instance-id &lt;GPU
instance IDs&gt; --id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Destroys GPU instances. The command fails if the requested GPU
instance is in use by an application.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>11) List compute instance profiles</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -lcip -gi &lt;GPU instance IDs&gt; -i &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --list-compute-instance-profiles
--gpu-instance-id &lt;GPU instance IDs&gt; --id &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Lists compute instance profiles, their availability and IDs.
Profiles describe the supported types of compute instances, including
all of the GPU resources they share or exclusively control.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>12) List compute instance possible placements</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -lcipp -gi &lt;GPU instance IDs&gt; -i &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --list-compute-instance-possible-placements
--gpu-instance-id &lt;GPU instance IDs&gt; --id &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Lists compute instance possible placements. Possible placements
describe the locations of the supported types of compute instances
within the GPU instance.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>13) Create compute instance</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -cci &lt;compute instance profile IDs or
names&gt; -gi &lt;GPU instance IDs&gt; -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --create-compute-instance &lt;compute instance
profile IDs or names&gt; --gpu-instance-id &lt;GPU instance IDs&gt; --id
&lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Creates compute instances for the given compute instance spcifiers.
A compute instance specifier comprises a compute instance profile name
or ID and an optional placement specifier consisting of a colon and a
placement start index. The command fails if the GPU resources required
to allocate the requested compute instances are not available, or if the
placement index is not valid for the given profile.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>14) List compute instances</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -lci -gi &lt;GPU instance IDs&gt; -i &lt;GPU
IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --list-compute-instances --gpu-instance-id
&lt;GPU instance IDs&gt; --id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Lists compute instances and their IDs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>15) Destroy compute instance</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig -dci -ci &lt;compute instance IDs&gt; -gi &lt;GPU
instance IDs&gt; -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi mig --destroy-compute-instance --compute-instance-id
&lt;compute instance IDs&gt; --gpu-instance-id &lt;GPU instance IDs&gt;
--id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Destroys compute instances. The command fails if the requested
compute instance is in use by an application.</dt>
<dd>

</dd>
</dl>
<h2>Boost Slider</h2>
<p>The privileged "nvidia-smi boost-slider" command-line is used to
manage boost slider on GPUs. It provides options to list and control
boost sliders.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Display help menu</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help menu for using the command-line.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) List one or more GPUs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider --id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects one or more GPUs using the given comma-separated GPU
indexes, PCI bus IDs or UUIDs. If not used, the given command-line
option applies to all of the supported GPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) List boost sliders</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider -l</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider --list</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>List all boost sliders for the selected devices.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Set video boost slider</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider --vboost &lt;value&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Set the video boost slider for the selected devices.</dt>
<dd>

</dd>
</dl>
<h2>Power Hint</h2>
<p>The privileged "nvidia-smi power-hint" command-line is used to query
power hint on GPUs.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Display help menu</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help menu for using the command-line.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) List one or more GPUs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider --id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects one or more GPUs using the given comma-separated GPU
indexes, PCI bus IDs or UUIDs. If not used, the given command-line
option applies to all of the supported GPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) List power hint info</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider -l</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider --list-info</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>List all boost sliders for the selected devices.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Query power hint</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider -gc &lt;value&gt; -t &lt;value&gt; -p
&lt;profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider --graphics-clock &lt;value&gt;
--temperature &lt;value&gt; --profile &lt;profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Query power hint with graphics clock, temperature and profile
id.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>5) Query power hint</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider -gc &lt;value&gt; -mc &lt;value&gt; -t
&lt;value&gt; -p &lt;profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi boost-slider --graphics-clock &lt;value&gt;
--memory-clock &lt;value&gt; --temperature &lt;value&gt; --profile
&lt;profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Query power hint with graphics clock, memory clock, temperature and
profile id.</dt>
<dd>

</dd>
</dl>
<h2>Confidential Compute</h2>
<p>The "nvidia-smi conf-compute" command-line is used to manage
confidential compute. It provides options to set and query confidential
compute.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Display help menu</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help menu for using the command-line.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) List one or more GPUs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute --id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects one or more GPUs using the given comma-separated GPU
indexes, PCI bus IDs or UUIDs. If not used, the given command-line
option applies to all of the supported GPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) Query confidential compute CPU capability</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -gc</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute --get-cpu-caps</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get confidential compute CPU capability.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Query confidential compute GPUs capability</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -gg</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute --get-gpus-caps</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get confidential compute GPUs capability.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>5) Query confidential compute devtools mode</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -d</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute --get-devtools-mode</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get confidential compute DevTools mode.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>6) Query confidential compute environment</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -e</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute --get-environment</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get confidential compute environment.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>7) Query confidential compute feature status</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -f</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute --get-cc-feature</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get confidential compute CC feature status.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>8) Query confidential compute GPU protected/unprotected
memory sizes</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -gm</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute --get-mem-size-info</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get confidential compute GPU protected/unprotected memory
sizes.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>9) Set confidential compute GPU unprotected memory
size</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -sm &lt;value&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute --set-unprotected-mem-size
&lt;value&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Set confidential compute GPU unprotected memory size in KiB.
Requires root.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>10) Set confidential compute GPUs ready state</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -srs &lt;value&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute --set-gpus-ready-state
&lt;value&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Set confidential compute GPUs ready state. The value must be 1 to
set the ready state and 0 to unset it. Requires root.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>11) Query confidential compute GPUs ready
state</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -grs</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute --get-gpus-ready-state</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get confidential compute GPUs ready state.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>12) Set Confidential Compute Key Rotation Max Attacker
Advantage</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -skr &lt;value&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute
--set-key-rotation-max-attacker-advantage</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Set Confidential Compute Key Rotation Max Attacker Advantage.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>13) Display Confidential Compute Key Rotation Threshold
Info</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -gkr</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute
--get-key-rotation-threshold-info</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display Confidential Compute Key Rotation Threshold Info.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>14) Display Confidential Compute Multi-GPU
Mode</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -mgm</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute --get-multigpu-mode</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display Confidential Compute Multi-GPU Mode.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>15) Display Confidential Compute Detailed Info</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute -q</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi conf-compute --query-conf-compute</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Display Confidential Compute Detailed Info.</dt>
<dd>

</dd>
</dl>
<h2>GPU Performance Monitoring(GPM) Stream State</h2>
<p>The "nvidia-smi gpm" command-line is used to manage GPU performance
monitoring unit. It provides options to query and set the stream
state.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Display help menu</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi gpm -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help menu for using the command-line.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) List one or more GPUs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi gpm -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi gpm --id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects one or more GPUs using the given comma-separated GPU
indexes, PCI bus IDs or UUIDs. If not used, the given command-line
option applies to all of the supported GPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) Query GPU performance monitoring stream
state</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi gpm -g</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi gpm --get-stream-state</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get gpm stream state for the selected devices.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Set GPU performance monitoring stream state</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi gpm -s &lt;value&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi gpm --set-stream-state &lt;value&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Set gpm stream state for the selected devices.</dt>
<dd>

</dd>
</dl>
<h2>GPU PCI section</h2>
<p>The "nvidia-smi pci" command-line is used to manage GPU PCI counters.
It provides options to query and clear PCI counters.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Display help menu</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi pci -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help menu for using the command-line.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Query PCI error counters</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi pci -i &lt;GPU index&gt; -gErrCnt</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Query PCI error counters of a GPU</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) Clear PCI error counters</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi pci -i &lt;GPU index&gt; -cErrCnt</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Clear PCI error counters of a GPU</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) Query PCI counters</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi pci -i &lt;GPU index&gt; -gCnt</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Query PCI RX and TX counters of a GPU</dt>
<dd>

</dd>
</dl>
<h2>Power Smoothing</h2>
<p>The "nvidia-smi power-smoothing" command-line is used to manage Power
Smoothing related data on the GPU. It provides options to set Power
Smoothing related data and query the preset profile definitions.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Display help menu</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help menu for using the command-line.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) List one or more GPUs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing --id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects one or more GPUs using the given comma-separated GPU
indexes, PCI bus IDs or UUIDs. If not used, the given command-line
option applies to all of the supported GPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) List one Preset Profile ID</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing -p &lt;Profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing --profile &lt;Profile
ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects a Preset Profile ID for which to update a value. This is
required when updating a Preset Profile parameter and prohibited in all
other cases.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Set Active Preset Profile ID</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing -spp &lt;Profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing --set-preset-profile &lt;Profile
ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Activate the deisred Preset Profile ID. Requires root.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Update percentage Total Module Power (TMP)
floor</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing -ptf &lt;Percentage&gt; -p
&lt;Profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing --percent-tmp-floor
&lt;Percentage&gt; --profile &lt;Profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Sets the percentage TMP floor to inputted value for a given Preset
Profile ID. The desired percentage should be from 0 - 100, given in the
form of "AB.CD", with a maximum of two decimal places of precision. For
example, to set value to 34.56%, user will input 34.56. Input can also
contain zero or one decimal places of precision. This option requires a
profile ID as an argument. Requires root.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Update Ramp-Up Rate</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing -rur &lt;value&gt; -p &lt;Profile
ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing --ramp-up-rate &lt;value&gt;
--profile &lt;Profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Sets the Ramp-Up Rate to the desired value for a given Preset
Profile ID. The rate given must be in the units of mW/s. This option
requires a profile ID as an argument. Requires root.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Update Ramp-Down Rate</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing -rdr &lt;value&gt; -p &lt;Profile
ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing --ramp-down-rate &lt;value&gt;
--profile &lt;Profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Sets the Ramp-Down Rate to the desired value for a given Preset
Profile ID. The rate given must be in the units of mW/s. This option
requires a profile ID as an argument. Requires root.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Update Ramp-Down Hysteresis</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing -rdh &lt;value&gt; -p &lt;Profile
ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing --ramp-down-hysteresis &lt;value&gt;
--profile &lt;Profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Sets the Ramp-Down Hysteresis to the desired value for a given
Preset Profile ID. The rate given must be in the units of ms. This
option requires a profile ID as an argument. Requires root.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Displays the Preset Profile definitions for all Profile
IDs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing -ppd</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing --print-profile-definitions</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays all values for each Preset Profile IDs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) Set Feature State</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing -s &lt;state&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-smoothing --state &lt;state&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Sets the state of the feature to either 0/DISABLED or 1/ENABLED.
Requires root.</dt>
<dd>

</dd>
</dl>
<h2> Power Profiles"</h2>
<p>The "nvidia-smi power-profiles" command-line is used to manage
Workload Power Profiles related data on the GPU. It provides options to
update Power Profiles data and query the supported Power Profiles.</p>
<p><strong>Usage:</strong></p>
<dl>
<dt><strong>1) Display help menu</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles -h</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Displays help menu for using the command-line.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>2) List one or more GPUs</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles -i &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles --id &lt;GPU IDs&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Selects one or more GPUs using the given comma-separated GPU
indexes, PCI bus IDs or UUIDs. If not used, the given command-line
option applies to all of the supported GPUs.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>3) List Power Profiles</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles -l</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles --list</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>List all Workload Power Profiles supported by the device.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>4) List Detailed Power Profiles info</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles -ld</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles --list-detailed</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>List all Workload Power Profiles supported by the device along with
their metadata. This includes the Profile ID, the Priority (where a
lower number indicates a higher priority), and Profiles that conflict
with the given profile. If two or more conflicting profiles are
requested, not all my be enforced.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>5) Get Requested Profiles</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles -gr</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles --get-requested</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get a list of all currently requested Power Profiles. Note that if
any of the profiles conflict, then not all may be enforced.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>6) Set Requested Profiles</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles -sr &lt;Profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles --set-requested &lt;Profile
ID(s)&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Adds the input profile(s) to the list of requested Power Profiles.
The input is a comma separated list of profile IDs with no spaces.
Requires root.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>7) Clear Requested Profiles</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles -cr &lt;Profile ID&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles --clear-requested &lt;Profile
ID(s)&gt;</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Removes the input profile(s) to the list of requested Power
Profiles. The input is a comma separated list of profile IDs with no
spaces. Requires root.</dt>
<dd>

</dd>
</dl>
<dl>
<dt><strong>8) Get Enforced Profiles</strong></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles -ge</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt><em>nvidia-smi power-profiles --get-enforced</em></dt>
<dd>

</dd>
</dl>
<dl>
<dt>Get a list of all currently enforced Power Profiles. Note that this
list may differ from the requested Profiles list if multiple conflicting
profiles are selected.</dt>
<dd>

</dd>
</dl>
<h2>GPU RUSD section</h2>
<p>The "nvidia-smi rusd" command-line is used to manage GPU RUSD
settings. It provides options to set RUSD settings. RUSD is Read only
User Shared Data buffer that keeps GPU metrics.</p>
<p><strong>Usage:</strong></p>
<p><strong>1) Display help menu</strong></p>
<p><strong>nvidia-smi rusd -h</strong></p>
<p>Displays help menu for using the command-line. Example:</p>
<pre><code>nvidia-smi rusd -h

    rusd -- RUSD settings section

    Usage: nvidia-smi rusd [options]

    Options include:
    [-h | --help]: Display help information
    [-i | --id]: Enumeration index, PCI bus ID or UUID.

    [-spm | --set-polling-mask]: Set polling mask for the given comma-separated list of metric groups
        Groups are &quot;none&quot;, &quot;clock&quot;, &quot;performance&quot;, &quot;memory&quot;, &quot;power&quot;, &quot;thermal&quot;, &quot;pci&quot;, &quot;fan&quot;, &quot;proc_util&quot;, &quot;all&quot;
</code></pre>
<p><strong>2) Set RUSD poll mask</strong></p>
<p><strong>nvidia-smi rusd -i &lt;GPU index&gt; -spm
&lt;mask_value&gt;</strong></p>
<p>Set RUSD poll mask Example:</p>
<pre><code>nvidia-smi rusd -spm all
nvidia-smi rusd -spm clock,performance
nvidia-smi rusd -spm none
</code></pre>
<h2>GPU PRM section</h2>
<p>The "nvidia-smi prm" command-line is used to read GPU PRM registers
and counters. This option is only available on GPUs based on NVIDIA
Blackwell or newer architectures.</p>
<p><strong>Usage:</strong></p>
<p><strong>1) Display help menu</strong></p>
<p><strong>nvidia-smi prm -h</strong></p>
<p>Displays the help menu for using the command-line. Example:</p>
<pre><code>nvidia-smi prm -h
    [-h | --help]: Display help information
    [-i | --index]: GPU index; mandatory if &quot;-n, --name&quot; is selected
    [-l | --list]: List all supported PRM registers and counters
    [-n | --name]: PRM Register name; mandatory if any of &quot;-f&quot; or &quot;-p&quot; are selected
    [-f | --info]: List all supported PRM parameters for the given register or counter
    [-p | --params]: PRM input parameters, if any; parameters are a comma-separated list of &lt;key&gt;=&lt;value&gt; pairs
</code></pre>
<p><strong>2) List supported PRM registers and counters</strong></p>
<p><strong>nvidia-smi prm --list</strong></p>
<p>Displays the list of supported GPU PRM registers and counters.
Example:</p>
<pre><code>nvidia-smi prm --list
Supported PRM registers:
            GHPKT
            MCAM
            MGIR
            MLPC
            MORD
            MPSCR
            MTCAP
            MTECR
            MTEIM
            MTEWE
            MTIE
            MTIM
            MTRC_CAP
            MTRC_CONF
            MTRC_CTRL
            MTSR
            PAOS
            PDDR
            PGUID
            PLIB
            PLTC
            PMAOS
            PMLP
            PMTU
            PPAOS
            PPCNT
            PPHCR
            PPLM
            PPLR
            PPRM
            PPRT
            PPSLC
            PPSLS
            PPTT
            PTYS
            SLRG
            SLTP
    Supported PRM counters:
            link_down_events
            oper_recovery
            plr_rcv_code_err
            plr_rcv_codes
            plr_rcv_uncorrectable_code
            plr_retry_codes
            plr_sync_events
            plr_xmit_codes
            plr_xmit_retry_events
            port_xmit_wait
            successful_recovery_events
            time_between_last_2_recoveries
            time_since_last_recovery
            total_successful_recovery_events
</code></pre>
<p><strong>3) List supported input parameters for a given PRM register
or counter</strong></p>
<p><strong>nvidia-smi prm -n &lt;register&gt; -f</strong> or
<strong>nvidia-smi prm -c &lt;counter&gt; -f</strong></p>
<p>Lists the supported input parameters (if any) for the given PRM
register or counter. Example:</p>
<pre><code>nvidia-smi prm -n PPCNT -f
Supported PRM parameters for register PPCNT:
        grp
        port_type
        lp_msb
        pnat
        local_port
        swid
        prio_tc
        grp_profile
        plane_ind
        counters_cap
        lp_gl
        clr
</code></pre>
<p>Note that some registers do not take any input parameters; in this
case the output of the above command will be <strong>'[NONE]'</strong>.
Example:</p>
<pre><code>nvidia-smi prm -n MGIR -f
Supported PRM parameters for register MGIR:
        [NONE]
</code></pre>
<p><strong>4) Read GPU PRM register</strong></p>
<p><strong>nvidia-smi prm -i &lt;GPU-index&gt; -n &lt;register&gt; -p
&lt;Comma-separated list of key EQUALS value pairs&gt;</strong></p>
<p>Reads the specified GPU PRM register with the given input parameters
and outputs to the screen. Note that the output may not include all
information in the register. Example:</p>
<pre><code>nvidia-smi prm -i 0 -n PPCNT -p=local_port=1,pnat=1,grp=35
PPCNT:
        grp = 35, port_type = 0, lp_msb = 0, pnat = 1, local_port = 1, swid = 0
        prio_tc = 0, grp_profile = 0, plane_ind = 0, counters_cap = 0, lp_gl = 0, clr = 0
</code></pre>
<p><strong>5) Read GPU PRM counter</strong></p>
<p><strong>nvidia-smi prm -i &lt;GPU-index&gt; -c &lt;counter&gt; -p
&lt;Comma-separated list of key EQUALS value pairs&gt;</strong></p>
<p>Reads the specified GPU PRM counter with the given input parameters
and outputs to the screen. Example:</p>
<pre><code>    nvidia-smi prm -i 0 -c plr_rcv_codes -p &quot;local_port=1&quot;
    plr_rcv_codes ==&gt; 0x64aace03ff
</code></pre>
<h2>System on Chip section</h2>
<p>The "nvidia-smi soc" command-line is used to manage system on chip
(SoC) metrics It provides options to query SoC metrics. This SoC section
is only available on Tegra Linux system.</p>
<p><strong>Usage:</strong></p>
<p><strong>1) Display help menu</strong></p>
<p><strong>nvidia-smi soc -h</strong></p>
<p>Displays help menu for using the command-line.</p>
<p>Example:</p>
<pre><code>nvidia-smi soc -h
    soc -- System on Chip section

    Usage: nvidia-smi soc [options]

    Options include:
    [-h | --help]: Display help information
    [-q | --query]: Query SoC metrics
</code></pre>
<p><strong>2) Query Soc Metrics</strong></p>
<p><strong>nvidia-smi soc -q</strong></p>
<p>Query SoC metrics.</p>
<p>Example:</p>
<pre><code>nvidia-smi soc -q

Memory:
    MemTotal: 128.83 GiB
    MemFree: 89.43 GiB
CPU:
    cpu0:
        clock: 972MHz
        utilization: 0%
    cpu1:
        clock: 972MHz
        utilization: 0%
    cpu2:
        clock: 972MHz
        utilization: 0%
    cpu3:
        clock: 972MHz
        utilization: 0%
    cpu4:
        clock: 972MHz
        utilization: 0%
    cpu5:
        clock: 972MHz
        utilization: 0%
    cpu6:
        clock: 972MHz
        utilization: 0%
    cpu7:
        clock: 972MHz
        utilization: 0%
    cpu8:
        clock: 1350MHz
        utilization: 0%
    cpu9:
        clock: 1674MHz
        utilization: 0%
    cpu10:
        clock: 972MHz
        utilization: 0%
    cpu11:
        clock: 972MHz
        utilization: 0%
    cpu12:
        clock: 972MHz
        utilization: 0%
    cpu13:
        clock: 972MHz
        utilization: 0%
Memory Controller:
    utilization: 0%
    clock: 4266MHz
Video Image Compositor:
    state: off
Programmable Vision Accelerator:
    state: off
Audio Processing Engine:
    Clock: 300 MHz
Thermal info:
    cpu-thermal: 59.22C
    tj-thermal: 60.41C
    soc012-thermal: 58.47C
    soc345-thermal: 60.41C
Power info:
    VDD_GPU: 5145 mW
    VDD_CPU_SOC_MSS: 5937 mW
    VIN_SYS_5V0: 4939 mW
</code></pre>
<h1>UNIT ATTRIBUTES</h1>
<p>The following list describes all possible data returned by the
<strong>-q -u</strong> unit query option. Unless otherwise noted all
numerical results are base 10 and unitless.</p>
<h2>Timestamp</h2>
<p>The current system timestamp at the time nvidia-smi was invoked.
Format is "Day-of-week Month Day HH:MM:SS Year".</p>
<h2>Driver Version</h2>
<p>The version of the installed NVIDIA display driver. Format is
"Major-Number.Minor-Number".</p>
<h2>HIC Info</h2>
<p>Information about any Host Interface Cards (HIC) that are installed
in the system.</p>
<dl>
<dt><strong>Firmware Version</strong></dt>
<dd>
<p>The version of the firmware running on the HIC.</p>
</dd>
</dl>
<h2>Attached Units</h2>
<p>The number of attached Units in the system.</p>
<h2>Product Name</h2>
<p>The official product name of the unit. This is an alphanumeric value.
For all S-class products.</p>
<h2>Product Id</h2>
<p>The product identifier for the unit. This is an alphanumeric value of
the form "part1-part2-part3". For all S-class products.</p>
<h2>Product Serial</h2>
<p>The immutable globally unique identifier for the unit. This is an
alphanumeric value. For all S-class products.</p>
<h2>Firmware Version</h2>
<p>The version of the firmware running on the unit. Format is
"Major-Number.Minor-Number". For all S-class products.</p>
<h2>LED State</h2>
<p>The LED indicator is used to flag systems with potential problems. An
LED color of AMBER indicates an issue. For all S-class products.</p>
<dl>
<dt><strong>Color</strong></dt>
<dd>
<p>The color of the LED indicator. Either "GREEN" or "AMBER".</p>
</dd>
</dl>
<dl>
<dt><strong>Cause</strong></dt>
<dd>
<p>The reason for the current LED color. The cause may be listed as any
combination of "Unknown", "Set to AMBER by host system", "Thermal sensor
failure", "Fan failure" and "Temperature exceeds critical limit".</p>
</dd>
</dl>
<h2>Temperature</h2>
<p>Temperature readings for important components of the Unit. All
readings are in degrees C. Not all readings may be available. For all
S-class products.</p>
<dl>
<dt><strong>Intake</strong></dt>
<dd>
<p>Air temperature at the unit intake.</p>
</dd>
</dl>
<dl>
<dt><strong>Exhaust</strong></dt>
<dd>
<p>Air temperature at the unit exhaust point.</p>
</dd>
</dl>
<dl>
<dt><strong>Board</strong></dt>
<dd>
<p>Air temperature across the unit board.</p>
</dd>
</dl>
<h2>PSU</h2>
<p>Readings for the unit power supply. For all S-class products.</p>
<dl>
<dt><strong>State</strong></dt>
<dd>
<p>Operating state of the PSU. The power supply state can be any of the
following: "Normal", "Abnormal", "High voltage", "Fan failure",
"Heatsink temperature", "Current limit", "Voltage below UV alarm
threshold", "Low-voltage", "I2C remote off command", "MOD_DISABLE input"
or "Short pin transition".</p>
</dd>
</dl>
<dl>
<dt><strong>Voltage</strong></dt>
<dd>
<p>PSU voltage setting, in volts.</p>
</dd>
</dl>
<dl>
<dt><strong>Current</strong></dt>
<dd>
<p>PSU current draw, in amps.</p>
</dd>
</dl>
<h2>Fan Info</h2>
<p>Fan readings for the unit. A reading is provided for each fan, of
which there can be many. For all S-class products.</p>
<dl>
<dt><strong>State</strong></dt>
<dd>
<p>The state of the fan, either "NORMAL" or "FAILED".</p>
</dd>
</dl>
<dl>
<dt><strong>Speed</strong></dt>
<dd>
<p>For a healthy fan, the fan's speed in RPM.</p>
</dd>
</dl>
<h2>Attached GPUs</h2>
<p>A list of PCI bus ids that correspond to each of the GPUs attached to
the unit. The bus ids have the form "domain:bus:device.function", in
hex. For all S-class products.</p>
<h1>NOTES</h1>
<p>On Linux, NVIDIA device files may be modified by nvidia-smi if run as
root. Please see the relevant section of the driver README file.</p>
<p>The <strong>-a</strong> and <strong>-g</strong> arguments are now
deprecated in favor of <strong>-q</strong> and <strong>-i</strong>,
respectively. However, the old arguments still work for this
release.</p>
<h1>EXAMPLES</h1>
<h2>nvidia-smi -q</h2>
<p>Query attributes for all GPUs once, and display in plain text to
stdout.</p>
<h2>nvidia-smi --format=csv,noheader
--query-gpu=uuid,persistence_mode</h2>
<p>Query UUID and persistence mode of all GPUs in the system.</p>
<h2>nvidia-smi -q -d ECC,POWER -i 0 -l 10 -f out.log</h2>
<p>Query ECC errors and power consumption for GPU 0 at a frequency of 10
seconds, indefinitely, and record to the file out.log.</p>
<h2> nvidia-smi -c 1 -i
GPU-b2f5f1b745e3d23d-65a3a26d-097db358-7303e0b6-149642ff3d219f8587cde3a8""</h2>
<p>Set the compute mode to "PROHIBITED" for GPU with UUID
"GPU-b2f5f1b745e3d23d-65a3a26d-097db358-7303e0b6-149642ff3d219f8587cde3a8".</p>
<h2>nvidia-smi -q -u -x --dtd</h2>
<p>Query attributes for all Units once, and display in XML format with
embedded DTD to stdout.</p>
<h2>nvidia-smi --dtd -u -f nvsmi_unit.dtd</h2>
<p>Write the Unit DTD to nvsmi_unit.dtd.</p>
<h2>nvidia-smi -q -d SUPPORTED_CLOCKS</h2>
<p>Display supported clocks of all GPUs.</p>
<h2>nvidia-smi -i 0 --applications-clocks 2500,745</h2>
<p>Set applications clocks to 2500 MHz memory, and 745 MHz graphics.</p>
<h2>nvidia-smi mig -cgi 19</h2>
<p>Create a MIG GPU instance on profile ID 19.</p>
<h2>nvidia-smi mig -cgi 19:2</h2>
<p>Create a MIG GPU instance on profile ID 19 at placement start index
2.</p>
<h2>nvidia-smi boost-slider -l</h2>
<p>List all boost sliders for all GPUs.</p>
<h2>nvidia-smi boost-slider --vboost 1</h2>
<p>Set vboost to value 1 for all GPUs.</p>
<h2>nvidia-smi power-hint -l</h2>
<p>List clock range, temperature range and supported profiles of power
hint.</p>
<h2>nvidia-smi boost-slider -gc 1350 -t 60 -p 0</h2>
<p>Query power hint with graphics clock at 1350MHz, temperature at 60C
and profile ID at 0.</p>
<h2>nvidia-smi boost-slider -gc 1350 -mc 1215 -t n5 -p 1</h2>
<p>Query power hint with graphics clock at 1350MHz, memory clock at
1216MHz, temperature at -5C and profile ID at 1.</p>
<h1>DEPRECATON AND REMOVAL NOTICES</h1>
<h2>Features deprecated and/or removed between nvidia-smi v580 Update
and v575</h2>
<h2></h2>
<ul>
<li><p>Removed deprecated graphics voltage value from Voltage section of
'nvidia-smi -q'</p></li>
<li><p>Removed deprecated GPU Reset Status from 'nvidia-smi -q'
output</p></li>
<li><p>Deprecated GPU Fabric State and Status from 'nvidia-smi
-q'</p></li>
</ul>
<h1>CHANGE LOG</h1>
<h2>Known Issues</h2>
<h2></h2>
<ul>
<li><p>On systems where GPUs are NUMA nodes, the accuracy of FB memory
utilization provided by nvidia-smi depends on the memory accounting of
the operating system. This is because FB memory is managed by the
operating system instead of the NVIDIA GPU driver. Typically, pages
allocated from FB memory are not released even after the process
terminates to enhance performance. In scenarios where the operating
system is under memory pressure, it may resort to utilizing FB memory.
Such actions can result in discrepancies in the accuracy of memory
reporting.</p></li>
<li><p>On Linux GPU Reset can't be triggered when there is pending GOM
change.</p></li>
<li><p>On Linux GPU Reset may not successfully change pending ECC mode.
A full reboot may be required to enable the mode change.</p></li>
<li><p>On Linux platforms that configure NVIDIA GPUs as NUMA nodes,
enabling persistence mode or resetting GPUs may print 'Warning:
persistence mode is disabled on device' if nvidia-persistenced is not
running, or if nvidia-persistenced cannot access files in the NVIDIA
driver's procfs directory for the device
(/proc/driver/nvidia/gpus/&lt;PCI config='' address&gt;=''&gt;/). During
GPU reset and driver reload, this directory will be deleted and
recreated, and outstanding references to the deleted directory, such as
mounts or shells, can prevent processes from accessing files in the new
directory.</p></li>
<li><p>There might be a slight discrepency between volatile/aggregate
ECC counters if recovery action was not taken</p></li>
<li><p>The GPU hostname commands are currently only supported on
compatible GB200 platforms.</p></li>
</ul>
<h2>Changes between nvidia-smi v590 Update and v580</h2>
<h2></h2>
<ul>
<li><p>Added support for inclusion of NIC data-direct devices in
<strong>'nvidia-smi topo -m'</strong></p></li>
<li><p>Added support to display System on Chip metrics via a new
command: <strong>'nvidia-smi soc' (support only on Tegra Linux
system)</strong></p></li>
<li><p>Added support for setting RUSD (Read only User Shared Data)
settings via a new command: <strong>'nvidia-smi rusd'</strong></p></li>
<li><p>Deprecated Applications Clocks, including:</p></li>
<li><p>Current Applications Clocks frequencies for Memory and Graphics
clocks</p></li>
<li><p>Default Applications Clocks frequencies for Memory and Graphics
clocks</p></li>
<li><p>The -ac option to set Applications Clocks frequencies for Memory
and Graphics clocks</p></li>
<li><p>The -rac option to reset Applications Clocks frequencies for
Memory and Graphics clocks</p></li>
<li><p>Added Nvlink version to 'nvidia-smi nvlink -info' output</p></li>
<li><p>Added new option 'nvidia-smi power-profiles -or' to set and
overwrite the requested power profiles.</p></li>
<li><p>Added new field 'EDPp Multipler' to 'nvidia-smi -q', which
expresses the EDPp ratio as a percentage.</p></li>
<li><p>Added new field '--query-gpu=edpp_multipler' to retrieve the
multipler.</p></li>
<li><p>Added Unrepairable memory status to ECC field: 'nvidia-smi -q -d
ECC'</p></li>
<li><p>Modified the 'FB Memory Usage', 'BAR1 Memory Usage' fields in the
'nvidia-smi -q' output to 'Shared FB Memory Usage', 'Shared BAR1 Usage'
respectively to indicate they are shared among the MIG devices
associated with the same GPU instance.</p></li>
<li><p>Added a new sub-option '-ei' to 'nvidia-smi vgpu -sl' to query
the vGPU software scheduler logs on the user provided engine.</p></li>
<li><p>Added new '--query-gpu' options for Delayed Power
Smoothing:</p></li>
</ul>
<dl>
<dt> </dt>
<dd>
<p>power_smoothing.supported</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.primary_power_floor</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.secondary_power_floor</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.min_primary_floor_activation_offset</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.min_primary_floor_activation_point</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.window_multiplier</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.curr_profile.secondary_power_floor</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.curr_profile.primary_floor_act_window_multiplier</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.curr_profile.primary_floor_tar_window_multiplier</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.curr_profile.primary_floor_act_offset</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.admin_override.secondary_power_floor</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.admin_override.primary_floor_act_window_multiplier</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.admin_override.primary_floor_tar_window_multiplier</p>
</dd>
<dt> </dt>
<dd>
<p>power_smoothing.admin_override.primary_floor_act_offset</p>
</dd>
</dl>
<ul>
<li><p>Added 4 new configurable profile fields in 'nvidia-smi
power-smoothing'.</p></li>
</ul>
<h2>Changes between nvidia-smi v580 Update and v575</h2>
<h2></h2>
<ul>
<li><p>Added Device NVLINK Encryption status in the new nvlink info
command 'nvidia-smi nvlink -info'</p></li>
<li><p>Added Muti-GPU mode NVLINK Encryption (NVLE) in 'nvidia-smi
conf-compute -mgm' and 'nvidia-smi conf-compute -q'</p></li>
<li><p>Added Nvlink Firmware Version info to the nvlink info command
'nvidia-smi nvlink -info'</p></li>
<li><p>Added Channel/TPC repair pending flags to ECC field: 'nvidia-smi
-q -d ECC'</p></li>
<li><p>Removed deprecated graphics voltage value from Voltage section of
'nvidia-smi -q'</p></li>
<li><p>Removed deprecated GPU Reset Status from 'nvidia-smi -q'
output</p></li>
<li><p>Added a new option to read GPU PRM registers: <strong>'nvidia-smi
prm'</strong></p></li>
<li><p>Added a new 'Bus' reset option to the existing reset command:
<strong>'nvidia-smi -r bus'</strong></p></li>
<li><p>Added a new output field called 'GPU PDI' to the 'nvidia-smi -q'
output</p></li>
<li><p>Added a new cmdline option '--columns' or '-col' to display the
summary in multi-column format.</p></li>
<li><p>Modified the 'Memory-Usage', 'BAR1-Usage' headers in the MIG
device table to 'Shared Memory-Usage', 'Shared BAR1-Usage' respectively
to indicate they are shared among the MIG devices associated with the
same GPU instance.</p></li>
<li><p>Updated GPU Fabric output from 'nvidia-smi -q' output:</p></li>
</ul>
<dl>
<dt> </dt>
<dd>
<p>Added Incorrect Configuration and Summary fields to Fabric Health
output</p>
</dd>
</dl>
<ul>
<li><p>Added support for NVIDIA Jetson Thor platform</p></li>
</ul>
<dl>
<dt> </dt>
<dd>
<p>Note that the following features are currently not supported on
Jetson Thor:</p>
</dd>
<dt> </dt>
<dd>
<p>Clock queries and commands</p>
</dd>
<dt> </dt>
<dd>
<p>Power queries and commands</p>
</dd>
<dt> </dt>
<dd>
<p>Thermal and temperature queries</p>
</dd>
<dt> </dt>
<dd>
<p>Per-process utilization via 'nvidia-smi pmon'</p>
</dd>
<dt> </dt>
<dd>
<p>SOC memory utilization</p>
</dd>
</dl>
<ul>
<li><p>Added new Incorrect Configuration Strings to Fabric Health
output</p></li>
</ul>
<dl>
<dt> </dt>
<dd>
<p>Incompatible Gpu Firmware</p>
</dd>
<dt> </dt>
<dd>
<p>Invalid Location</p>
</dd>
</dl>
<ul>
<li><p>Added new command line options '--get-hostname' and
'--set-hostname' to get and set GPU hostnames, respectively.</p></li>
<li><p>Added a new command to read GPU PRM counters: <strong>'nvidia-smi
prm -c'</strong></p></li>
</ul>
<h2>Changes between nvidia-smi v575 Update and v570</h2>
<h2></h2>
<ul>
<li><p>Added new --query-gpu option inforom.checksum_validation to check
the inforom checksum validation (nvidia-smi --query-gpu
inforom.checksum_validation)</p></li>
<li><p>Updated 'nvidia-smi -q' to print both 'Instantaneous Power Draw'
and 'Average Power Draw' in all cases where 'Power Draw' used to be
printed.</p></li>
<li><p>Added support to nvidia-smi c2c -e to display C2C Link
Errors</p></li>
<li><p>Added support to nvidia-smi c2c -gLowPwrInfo to display C2C Link
Power state</p></li>
<li><p>Added new fields for Clock Event Reason Counters which can be
queries with 'nvidia-smi -q' or with the 'nvidia-smi -q -d PERFORMANCE'
display flag.</p></li>
<li><p>Added new query GPU options for Clock Event Reason Counters:
'nvidia-smi
--query-gpu=clocks_event_reasons_counters.{sw_power_cap,sw_thermal_slowdown,sync_boost,hw_thermal_slowdown,hw_power_brake_slowdown}'</p></li>
<li><p>Added new fields for MIG timeslicing which can be queried with
'nvidia-smi -q'</p></li>
<li><p>Added a new cmdline option '-smts' to 'nvidia-smi vgpu' to set
vGPU MIG timeslice mode</p></li>
<li><p>Added a new sub-option '-gi' to 'nvidia-smi vgpu -c' to query the
currently creatable vGPU types on the user provided GPU
Instance</p></li>
<li><p>Added a new sub-option '-gi' to 'nvidia-smi vgpu -q' to query
detailed information of the currently active vGPU instances on the user
provided GPU Instance</p></li>
<li><p>Added a new sub-option '-gi' to 'nvidia-smi vgpu -ss' to query
the vGPU software scheduler state on the user provided GPU
Instance</p></li>
<li><p>Added a new sub-option '-gi' to 'nvidia-smi vgpu -sl' to query
the vGPU software scheduler logs on the user provided GPU
Instance</p></li>
<li><p>Added a new cmdline option '-ghm' to 'nvidia-smi vgpu' to get
vGPU heterogeneous mode on the user provided GPU Instance</p></li>
<li><p>Added a new sub-option '-gi' to 'nvidia-smi vgpu -shm' to set the
vGPU heterogeneous mode on the user provided GPU Instance</p></li>
<li><p>Added new field for max instances per GPU Instance which can be
queried with 'nvidia-smi vgpu -s -v'</p></li>
<li><p>Added a new sub-option '-gi' to 'nvidia-smi vgpu
set-scheduler-state' to set the vGPU software scheduler state on the
user provided GPU Instance.</p></li>
<li><p>Added a new sub-option '-gi' to 'nvidia-smi vgpu -c -v' to query
detailed information of the creatable vGPU types on the user provided
GPU Instance</p></li>
<li><p>Added a new cmdlin option
'--query-gpu-instance-vgpu-scheduler-logs' to 'nvidia-smi vgpu' to get
the vGPU software scheduler logs on the user provided GPU Instance in
CSV format. See nvidia-smi vgpu
--help-gpu-instance-vgpu-query-scheduler-logs for details.</p></li>
</ul>
<h2>Changes between nvidia-smi v570 Update and v565</h2>
<h2></h2>
<ul>
<li><p>Added new cmdline option '-\sLWidth' and '-\gLWidth' to
'nvidia-smi nvlink'</p></li>
<li><p>Added new ability to display Nvlink sleep state with 'nvidia-smi
nvlink -\s for Blackwell and onward generations'</p></li>
<li><p>Added new query GPU options for average/instant module power
draw: 'nvidia-smi
--query-gpu=module.power.draw.{average,instant}'</p></li>
<li><p>Added new query GPU options for default/max/min module power
limits: 'nvidia-smi
--query-gpu=module.power.{default_limit,max_limit,min_limit}'</p></li>
<li><p>Added new query GPU options for module power limits: 'nvidia-smi
--query-gpu=module.power.limit'</p></li>
<li><p>Added new query GPU options for enforced module power limits:
'nvidia-smi --query-gpu=module.enforced.power.limit'</p></li>
<li><p>Added new query GPU aliases for GPU Power options</p></li>
<li><p>Added a new command to get confidential compute info: 'nvidia-smi
conf-compute -q'</p></li>
<li><p>Added new Power Profiles section in nvidia-smi -q and
corresponding -d display flag POWER_PROFILES</p></li>
<li><p>Added new Power Profiles option 'nvidia-smi power-profiles' to
get/set power profiles related information.</p></li>
<li><p>Added the platform information query to 'nvidia-smi -q'</p></li>
<li><p>Added the platform information query to 'nvidia-smi --query-gpu
platform'</p></li>
<li><p>Added new Power Smoothing option 'nvidia-smi power-smoothing' to
set power smoothing related values.</p></li>
<li><p>Added new Power Smoothing section in nvidia-smi -q and
corresponding -d display flag POWER_SMOOTHING</p></li>
<li><p>Deprecated graphics voltage value from Voltage section of
nvidia-smi -q. Voltage now always displays as 'N/A' and will be removed
in a future release.</p></li>
<li><p>Added new topo option nvidia-smi topo -nvme to display GPUs vs
NVMes connecting path.</p></li>
<li><p>Changed help string for the command 'nvidia-smi topo -p2p -p'
from 'prop' to 'pcie' to better describe the p2p capability.</p></li>
<li><p>Added new command 'nvidia-smi pci -gCnt' to query PCIe RX/TX
Bytes.</p></li>
<li><p>Added EGM capability display under new Capabilities section in
nvidia-smi -q command.</p></li>
<li><p>Add multiGpuMode dipsplay via nvidia-smi via 'nvidia-smi
conf-compute --get-multigpu-mode' or 'nvidia-smi conf-compute
-mgm'</p></li>
<li><p>GPU Reset Status in nvidia-smi -q has been deprecated. GPU
Recovery action provides all the necessary actions</p></li>
<li><p>nvidia-smi -q will now display Dram encryption state</p></li>
<li><p>nvidia-smi -den/--dram-encryption 0/1 to disable/enable dram
encryption</p></li>
<li><p>Added new status to nvidia fabric health. nvidia-smi -q will
display 3 new fields in Fabric Health - Route Recovery in progress,
Route Unhealthy and Access Timeout Recovery</p></li>
<li><p>In nvidia-smi -q Platform Info - RACK GUID is changed to Platform
Info - RACK Serial Number</p></li>
<li><p>In nvidia-smi --query-gpu new option for gpu_recovery_action is
added</p></li>
<li><p>Added new counters for Nvlink5 in nvidia-smi nvlink -e:</p></li>
</ul>
<dl>
<dt> </dt>
<dd>
<p>Effective Errors to get sum of the number of errors in each Nvlink
packet</p>
</dd>
<dt> </dt>
<dd>
<p>Effective BER to get Effective BER for effective errors</p>
</dd>
<dt> </dt>
<dd>
<p>FEC Errors - 0 to 15 to get count of symbol errors that are
corrected</p>
</dd>
</dl>
<ul>
<li><p>Added a new output field called 'GPU Fabric GUID' to the
'nvidia-smi -q' output</p></li>
<li><p>Added a new property called 'platform.gpu_fabric_guid' to
'nvidia-smi --query-gpu'</p></li>
<li><p>Updated 'nvidia-smi nvlink -gLowPwrInfo' command to display the
Power Threshold Range and Units</p></li>
</ul>
<h2>Changes between nvidia-smi v565 Update and v560</h2>
<h2></h2>
<ul>
<li><p>Added the reporting of vGPU homogeneous mode to 'nvidia-smi
-q'.</p></li>
<li><p>Added the reporting of homogeneous vGPU placements to 'nvidia-smi
vgpu -s -v', complementing the existing reporting of heterogeneous vGPU
placements.</p></li>
</ul>
<h2>Changes between nvidia-smi v560 Update and v555</h2>
<h2></h2>
<ul>
<li><p>Added 'Atomic Caps Inbound' in the PCI section of 'nvidia-smi
-q'.</p></li>
<li><p>Updated ECC and row remapper output for options '--query-gpu' and
'--query-remapped-rows'.</p></li>
<li><p>Added support for events including ECC single-bit error storm,
DRAM retirement, DRAM retirement failure, contained/nonfatal poison and
uncontained/fatal poison.</p></li>
<li><p>Added support in 'nvidia-smi nvlink -e' to display NVLink5 error
counters</p></li>
</ul>
<h2>Changes between nvidia-smi v550 Update and v545</h2>
<h2></h2>
<ul>
<li><p>Added a new cmdline option to print out version information:
--version</p></li>
<li><p>Added ability to print out only the GSP firmware version
with'nvidia-smi -q -d'. Example commandline: nvidia-smi -q -d
GSP_FIRMWARE_VERSION</p></li>
<li><p>Added support to query pci.baseClass and pci.subClass. See
nvidia-smi --help-query-gpu for details.</p></li>
<li><p>Added PCI base and sub classcodes to 'nvidia-smi -q'
output.</p></li>
<li><p>Added new cmdline option '--format' to 'nvidia-smi dmon' to
support 'csv', 'nounit' and 'noheader' format specifiers</p></li>
<li><p>Added a new cmdline option '--gpm-options' to 'nvidia-smi dmon'
to support GPM metrics report in MIG mode</p></li>
<li><p>Added the NVJPG and NVOFA utilization report to 'nvidia-smi
pmon'</p></li>
<li><p>Added the NVJPG and NVOFA utilization report to 'nvidia-smi -q -d
utilization'</p></li>
<li><p>Added the NVJPG and NVOFA utilization report to 'nvidia-smi vgpu
-q' to report NVJPG/NVOFA utilization on active vgpus</p></li>
<li><p>Added the NVJPG and NVOFA utilization report to 'nvidia-smi vgpu
-u' to periodically report NVJPG/NVOFA utilization on active
vgpus</p></li>
<li><p>Added the NVJPG and NVOFA utilization report to 'nvidia-smi vgpu
-p' to periodically report NVJPG/NVOFA utilization on running processs
of active vgpus</p></li>
<li><p>Added a new cmdline option '-shm' to 'nvidia-smi vgpu' to set
vGPU heterogeneous mode</p></li>
<li><p>Added the reporting of vGPU heterogeneous mode in 'nvidia-smi
-q'</p></li>
<li><p>Added ability to call 'nvidia-smi mig -lgip' and 'nvidia-smi mig
-lgipp' to work without requiring MIG being enabled</p></li>
<li><p>Added support to query confidential compute key rotation
threshold info.</p></li>
<li><p>Added support to set confidential compute key rotation max
attacker advantage.</p></li>
<li><p>Added a new cmdline option '--sparse-operation-mode' to
'nvidia-smi clocks' to set the sparse operation mode</p></li>
<li><p>Added the reporting of sparse operation mode to 'nvidia-smi -q -d
PERFORMANCE'</p></li>
</ul>
<h2>Changes between nvidia-smi v535 Update and v545</h2>
<h2></h2>
<ul>
<li><p>Added support to query the timestamp and duration of the latest
flush of the BBX object to the inforom storage.</p></li>
<li><p>Added support for reporting out GPU Memory power usage.</p></li>
</ul>
<h2>Changes between nvidia-smi v535 Update and v530</h2>
<h2></h2>
<ul>
<li><p>Updated the SRAM error status reported in the ECC query
'nvidia-smi -q -d ECC'</p></li>
<li><p>Added support to query and report the GPU JPEG and OFA (Optical
Flow Accelerator) utilizations.</p></li>
<li><p>Removed deprecated 'stats' command.</p></li>
<li><p>Added support to set the vGPU software scheduler state.</p></li>
<li><p>Renamed counter collection unit to gpu performance
monitoring.</p></li>
<li><p>Added new C2C Mode reporting to device query.</p></li>
<li><p>Added back clock_throttle_reasons to --query-gpu to not break
backwards compatibility</p></li>
<li><p>Added support to get confidential compute CPU capability and GPUs
capability.</p></li>
<li><p>Added support to set confidential compute unprotected memory and
GPU ready state.</p></li>
<li><p>Added support to get confidential compute memory info and GPU
ready state.</p></li>
<li><p>Added support to display confidential compute devtools mode,
environment and feature status.</p></li>
</ul>
<h2>Changes between nvidia-smi v525 Update and v530</h2>
<h2></h2>
<ul>
<li><p>Added support to query power.draw.average and power.draw.instant.
See nvidia-smi --help-query-gpu for details.</p></li>
<li><p>Added support to get the vGPU software scheduler state.</p></li>
<li><p>Added support to get the vGPU software scheduler logs.</p></li>
<li><p>Added support to get the vGPU software scheduler
capabilities.</p></li>
<li><p>Renamed Clock Throttle Reasons to Clock Event Reasons.</p></li>
</ul>
<h2>Changes between nvidia-smi v520 Update and v525</h2>
<h2></h2>
<ul>
<li><p>Added support to query and set counter collection unit stream
state.</p></li>
</ul>
<h2>Changes between nvidia-smi v470 Update and v510</h2>
<h2></h2>
<ul>
<li><p>Add new 'Reserved' memory reporting to the FB memory
output</p></li>
</ul>
<h2>Changes between nvidia-smi v465 Update and v470</h2>
<h2></h2>
<ul>
<li><p>Added support to query power hint</p></li>
</ul>
<h2>Changes between nvidia-smi v460 Update and v465</h2>
<h2></h2>
<ul>
<li><p>Removed support for -acp,--application-clock-permissions
option</p></li>
</ul>
<h2>Changes between nvidia-smi v450 Update and v460</h2>
<h2></h2>
<ul>
<li><p>Add option to specify placement when creating a MIG GPU
instance.</p></li>
<li><p>Added support to query and control boost slider</p></li>
</ul>
<h2>Changes between nvidia-smi v445 Update and v450</h2>
<h2></h2>
<ul>
<li><p>Added --lock-memory-clock and --reset-memory-clock command to
lock to closest min/max Memory clock provided and ability to reset
Memory clock</p></li>
<li><p>Allow fan speeds greater than 100% to be reported</p></li>
<li><p>Added topo support to display NUMA node affinity for GPU
devices</p></li>
<li><p>Added support to create MIG instances using profile
names</p></li>
<li><p>Added support to create the default compute instance while
creating a GPU instance</p></li>
<li><p>Added support to query and disable MIG mode on Windows</p></li>
<li><p>Removed support of GPU reset(-r) command on MIG enabled vGPU
guests</p></li>
</ul>
<h2>Changes between nvidia-smi v418 Update and v445</h2>
<h2></h2>
<ul>
<li><p>Added support for Multi Instance GPU (MIG)</p></li>
<li><p>Added support to individually reset NVLink-capable GPUs based on
the NVIDIA Ampere architecture</p></li>
</ul>
<h2>Changes between nvidia-smi v361 Update and v418</h2>
<h2></h2>
<ul>
<li><p>Support for Volta and Turing architectures, bug fixes,
performance improvements, and new features</p></li>
</ul>
<h2>Changes between nvidia-smi v352 Update and v361</h2>
<h2></h2>
<ul>
<li><p>Added nvlink support to expose the publicly available NVLINK NVML
APIs</p></li>
<li><p>Added clocks sub-command with synchronized boost support</p></li>
<li><p>Updated nvidia-smi stats to report GPU temperature
metric</p></li>
<li><p>Updated nvidia-smi dmon to support PCIe throughput</p></li>
<li><p>Updated nvidia-smi daemon/replay to support PCIe
throughput</p></li>
<li><p>Updated nvidia-smi dmon, daemon and replay to support PCIe Replay
Errors</p></li>
<li><p>Added GPU part numbers in nvidia-smi -q</p></li>
<li><p>Removed support for exclusive thread compute mode</p></li>
<li><p>Added Video (encoder/decode) clocks to the Clocks and Max Clocks
display of nvidia-smi -q</p></li>
<li><p>Added memory temperature output to nvidia-smi dmon</p></li>
<li><p>Added --lock-gpu-clock and --reset-gpu-clock command to lock to
closest min/max GPU clock provided and reset clock</p></li>
<li><p>Added --cuda-clocks to override or restore default CUDA
clocks</p></li>
</ul>
<h2>Changes between nvidia-smi v346 Update and v352</h2>
<h2></h2>
<ul>
<li><p>Added topo support to display affinities per GPU</p></li>
<li><p>Added topo support to display neighboring GPUs for a given
level</p></li>
<li><p>Added topo support to show pathway between two given
GPUs</p></li>
<li><p>Added 'nvidia-smi pmon' command-line for process monitoring in
scrolling format</p></li>
<li><p>Added '--debug' option to produce an encrypted debug log for use
in submission of bugs back to NVIDIA</p></li>
<li><p>Fixed reporting of Used/Free memory under Windows WDDM
mode</p></li>
<li><p>The accounting stats is updated to include both running and
terminated processes. The execution time of running process is reported
as 0 and updated to actual value when the process is
terminated.</p></li>
</ul>
<h2>Changes between nvidia-smi v340 Update and v346</h2>
<h2></h2>
<ul>
<li><p>Added reporting of PCIe replay counters</p></li>
<li><p>Added support for reporting Graphics processes via
nvidia-smi</p></li>
<li><p>Added reporting of PCIe utilization</p></li>
<li><p>Added dmon command-line for device monitoring in scrolling
format</p></li>
<li><p>Added daemon command-line to run in background and monitor
devices as a daemon process. Generates dated log files at
/var/log/nvstats/</p></li>
<li><p>Added replay command-line to replay/extract the stat files
generated by the daemon tool</p></li>
</ul>
<h2>Changes between nvidia-smi v331 Update and v340</h2>
<h2></h2>
<ul>
<li><p>Added reporting of temperature threshold information.</p></li>
<li><p>Added reporting of brand information (e.g. Tesla, Quadro,
etc.)</p></li>
<li><p>Added support for K40d and K80.</p></li>
<li><p>Added reporting of max, min and avg for samples (power,
utilization, clock changes). Example commandline: nvidia-smi -q -d
power,utilization, clock</p></li>
<li><p>Added nvidia-smi stats interface to collect statistics such as
power, utilization, clock changes, xid events and perf capping counters
with a notion of time attached to each sample. Example commandline:
nvidia-smi stats</p></li>
<li><p>Added support for collectively reporting metrics on more than one
GPU. Used with comma separated with '-i' option. Example: nvidia-smi -i
0,1,2</p></li>
<li><p>Added support for displaying the GPU encoder and decoder
utilizations</p></li>
<li><p>Added nvidia-smi topo interface to display the GPUDirect
communication matrix (EXPERIMENTAL)</p></li>
<li><p>Added support for displayed the GPU board ID and whether or not
it is a multiGPU board</p></li>
<li><p>Removed user-defined throttle reason from XML output</p></li>
</ul>
<h2>Changes between nvidia-smi v5.319 Update and v331</h2>
<h2></h2>
<ul>
<li><p>Added reporting of minor number.</p></li>
<li><p>Added reporting BAR1 memory size.</p></li>
<li><p>Added reporting of bridge chip firmware.</p></li>
</ul>
<h2>Changes between nvidia-smi v4.319 Production and v4.319 Update</h2>
<h2></h2>
<ul>
<li><p>Added new --applications-clocks-permission switch to change
permission requirements for setting and resetting applications
clocks.</p></li>
</ul>
<h2>Changes between nvidia-smi v4.304 and v4.319 Production</h2>
<h2></h2>
<ul>
<li><p>Added reporting of Display Active state and updated documentation
to clarify how it differs from Display Mode and Display Active
state</p></li>
<li><p>For consistency on multi-GPU boards nvidia-smi -L always displays
UUID instead of serial number</p></li>
<li><p>Added machine readable selective reporting. See SELECTIVE QUERY
OPTIONS section of nvidia-smi -h</p></li>
<li><p>Added queries for page retirement information. See
--help-query-retired-pages and -d PAGE_RETIREMENT</p></li>
<li><p>Renamed Clock Throttle Reason User Defined Clocks to Applications
Clocks Setting</p></li>
<li><p>On error, return codes have distinct non zero values for each
error class. See RETURN VALUE section</p></li>
<li><p>nvidia-smi -i can now query information from healthy GPU when
there is a problem with other GPU in the system</p></li>
<li><p>All messages that point to a problem with a GPU print pci bus id
of a GPU at fault</p></li>
<li><p>New flag --loop-ms for querying information at higher rates than
once a second (can have negative impact on system performance)</p></li>
<li><p>Added queries for accounting procsses. See
--help-query-accounted-apps and -d ACCOUNTING</p></li>
<li><p>Added the enforced power limit to the query output</p></li>
</ul>
<h2>Changes between nvidia-smi v4.304 RC and v4.304 Production</h2>
<h2></h2>
<ul>
<li><p>Added reporting of GPU Operation Mode (GOM)</p></li>
<li><p>Added new --gom switch to set GPU Operation Mode</p></li>
</ul>
<h2>Changes between nvidia-smi v3.295 and v4.304 RC</h2>
<h2></h2>
<ul>
<li><p>Reformatted non-verbose output due to user feedback. Removed
pending information from table.</p></li>
<li><p>Print out helpful message if initialization fails due to kernel
module not receiving interrupts</p></li>
<li><p>Better error handling when NVML shared library is not present in
the system</p></li>
<li><p>Added new --applications-clocks switch</p></li>
<li><p>Added new filter to --display switch. Run with -d
SUPPORTED_CLOCKS to list possible clocks on a GPU</p></li>
<li><p>When reporting free memory, calculate it from the rounded total
and used memory so that values add up</p></li>
<li><p>Added reporting of power management limit constraints and default
limit</p></li>
<li><p>Added new --power-limit switch</p></li>
<li><p>Added reporting of texture memory ECC errors</p></li>
<li><p>Added reporting of Clock Throttle Reasons</p></li>
</ul>
<h2>Changes between nvidia-smi v2.285 and v3.295</h2>
<h2></h2>
<ul>
<li><p>Clearer error reporting for running commands (like changing
compute mode)</p></li>
<li><p>When running commands on multiple GPUs at once N/A errors are
treated as warnings.</p></li>
<li><p>nvidia-smi -i now also supports UUID</p></li>
<li><p>UUID format changed to match UUID standard and will report a
different value.</p></li>
</ul>
<h2>Changes between nvidia-smi v2.0 and v2.285</h2>
<h2></h2>
<ul>
<li><p>Report VBIOS version.</p></li>
<li><p>Added -d/--display flag to filter parts of data</p></li>
<li><p>Added reporting of PCI Sub System ID</p></li>
<li><p>Updated docs to indicate we support M2075 and C2075</p></li>
<li><p>Report HIC HWBC firmware version with -u switch</p></li>
<li><p>Report max(P0) clocks next to current clocks</p></li>
<li><p>Added --dtd flag to print the device or unit DTD</p></li>
<li><p>Added message when NVIDIA driver is not running</p></li>
<li><p>Added reporting of PCIe link generation (max and current), and
link width (max and current).</p></li>
<li><p>Getting pending driver model works on non-admin</p></li>
<li><p>Added support for running nvidia-smi on Windows Guest
accounts</p></li>
<li><p>Running nvidia-smi without -q command will output non verbose
version of -q instead of help</p></li>
<li><p>Fixed parsing of -l/--loop= argument (default value, 0, to big
value)</p></li>
<li><p>Changed format of pciBusId (to XXXX:XX:XX.X - this change was
visible in 280)</p></li>
<li><p>Parsing of busId for -i command is less restrictive. You can pass
0:2:0.0 or 0000:02:00 and other variations</p></li>
<li><p>Changed versioning scheme to also include 'driver
version'</p></li>
<li><p>XML format always conforms to DTD, even when error conditions
occur</p></li>
<li><p>Added support for single and double bit ECC events and XID errors
(enabled by default with -l flag disabled for -x flag)</p></li>
<li><p>Added device reset -r --gpu-reset flags</p></li>
<li><p>Added listing of compute running processes</p></li>
<li><p>Renamed power state to performance state. Deprecated support
exists in XML output only.</p></li>
<li><p>Updated DTD version number to 2.0 to match the updated XML output
</p></li>
</ul>
<h1>SEE ALSO</h1>
<p>On Linux, the driver README is installed as
/usr/share/doc/NVIDIA_GLX-1.0/README.txt</p>
<h1>AUTHOR</h1>
<p>NVIDIA Corporation</p>
<h1>COPYRIGHT</h1>
<p>Copyright 2011-2025 NVIDIA Corporation</p>
